# -*- coding: utf-8 -*-
"""
æ•°æ®é›†å¤„ç†å™¨ï¼šç»Ÿä¸€ä¸åŒæ•°æ®é›†çš„åŠ è½½ã€åˆ’åˆ†å’Œæ ¼å¼åŒ–
"""

import os
import json
import hashlib
from abc import ABC, abstractmethod
from typing import Dict, List, Tuple, Optional
from pathlib import Path
import numpy as np
# from datasets import load_dataset, Dataset, DatasetDict, get_dataset_config_names
from datasets import load_dataset, Dataset, DatasetDict, get_dataset_config_names, concatenate_datasets
try:
    from modelscope.msdatasets import MsDataset
except Exception:
    MsDataset = None

class BaseDatasetHandler(ABC):
    """æ•°æ®é›†å¤„ç†å™¨åŸºç±»"""

    def __init__(self, dataset_name: str, subset: str = None, cache_dir: str = "./dataset_splits"):
        self.dataset_name = dataset_name
        self.subset = subset
        self.cache_dir = cache_dir
        Path(cache_dir).mkdir(parents=True, exist_ok=True)

    @abstractmethod
    def load_and_split(self, test_size: int = 300, seed: int = 42) -> Tuple[Dataset, Dataset]:
        """åŠ è½½æ•°æ®é›†å¹¶è¿”å› (train, test)"""
        pass

    @abstractmethod
    def format_example_io(self, example: Dict) -> Tuple[str, str]:
        """æ ¼å¼åŒ–ä¸º I/O æ¨¡å¼çš„ (question, answer)"""
        pass

    @abstractmethod
    def format_example_cot(self, example: Dict) -> Tuple[str, str]:
        """æ ¼å¼åŒ–ä¸º COT æ¨¡å¼çš„ (question, answer_with_reasoning)"""
        pass

    @abstractmethod
    def extract_gold_answer(self, example: Dict) -> str:
        """æå–æ ‡å‡†ç­”æ¡ˆ"""
        pass

    @abstractmethod
    def extract_prediction(self, model_output: str) -> str:
        """ä»æ¨¡å‹è¾“å‡ºä¸­æå–é¢„æµ‹ç­”æ¡ˆ"""
        pass

    def _get_split_cache_path(self, test_size: int, seed: int) -> str:
        """ç”Ÿæˆæ•°æ®é›†åˆ’åˆ†çš„ç¼“å­˜è·¯å¾„"""
        cache_id = f"{self.dataset_name}_{self.subset or 'default'}_{test_size}_{seed}"
        cache_hash = hashlib.md5(cache_id.encode()).hexdigest()[:8]
        return os.path.join(self.cache_dir, f"split_{cache_hash}.json")

    def _save_split_indices(self, train_indices: List[int], test_indices: List[int], path: str):
        """ä¿å­˜æ•°æ®é›†åˆ’åˆ†ç´¢å¼•"""
        with open(path, 'w') as f:
            json.dump({
                'train_indices': train_indices,
                'test_indices': test_indices
            }, f)

    def _load_split_indices(self, path: str) -> Optional[Tuple[List[int], List[int]]]:
        """åŠ è½½æ•°æ®é›†åˆ’åˆ†ç´¢å¼•"""
        if os.path.exists(path):
            with open(path, 'r') as f:
                data = json.load(f)
                return data['train_indices'], data['test_indices']
        return None

    @staticmethod
    def get_available_subsets(dataset_name: str) -> List[str]:
        """è·å–æ•°æ®é›†çš„æ‰€æœ‰å¯ç”¨å­é›†"""
        try:
            subsets = get_dataset_config_names(dataset_name)
            return subsets
        except Exception as e:
            print(f"Error getting subsets for {dataset_name}: {e}")
            return []


class GSM8KHandler(BaseDatasetHandler):
    """GSM8K æ•°æ®é›†å¤„ç†å™¨"""

    def load_and_split(self, test_size: int = 300, seed: int = 42) -> Tuple[Dataset, Dataset]:
        # GSM8K é»˜è®¤ä½¿ç”¨ main subset
        subset = self.subset or "main"
        dataset = load_dataset(self.dataset_name, subset)

        # GSM8K å·²æœ‰ train/test åˆ’åˆ†
        train = dataset["train"]
        test = dataset["test"]
        if test_size < len(test):
            # ä¿è¯ç›¸åŒçš„seedå¾—åˆ°ç›¸åŒçš„æµ‹è¯•é›†
            np.random.seed(seed)
            test_indices = np.random.choice(len(test), test_size, replace=False).tolist()
            test_indices.sort()  # æ’åºç¡®ä¿é¡ºåºä¸€è‡´
            test = test.select(test_indices)
        return train, test

    def format_example_io(self, example: Dict) -> Tuple[str, str]:
        q = example["question"].strip()
        a_raw = example["answer"].strip()
        final_answer = a_raw.split("####")[-1].strip()
        return q, f"#### {final_answer}"

    def format_example_cot(self, example: Dict) -> Tuple[str, str]:
        q = example["question"].strip()
        a = example["answer"].strip()
        return q, a

    def extract_gold_answer(self, example: Dict) -> str:
        return example["answer"].split("####")[-1].strip()

    def extract_prediction(self, model_output: str) -> str:
        """
        ä»æ¨¡å‹è¾“å‡ºä¸­æå–é¢„æµ‹ç­”æ¡ˆ
        
        æ”¯æŒå¤šç§æ ¼å¼:
        1. #### åè·Ÿæ•°å­—: "#### 8" æˆ– "####8"
        2. æ•°å­—åè·Ÿ ####: "$57.00.\n####" -> æå– $57.00 ä¹‹å‰çš„æœ€åä¸€ä¸ªæ•°å­—
        3. å¸¦é€—å·çš„æ•°å­—: "$8,400" -> 8400
        4. å¸¦ç©ºæ ¼çš„æ•°å­—: "$9 500" -> 9500
        """
        import re
        text = model_output.strip()
        
        # è¾…åŠ©å‡½æ•°ï¼šæ¸…ç†æ•°å­—å­—ç¬¦ä¸²ï¼ˆç§»é™¤é€—å·å’Œç©ºæ ¼ï¼‰
        def clean_number(num_str: str) -> str:
            return re.sub(r'[,\s]', '', num_str)
        
        # è¾…åŠ©å‡½æ•°ï¼šæå–æ•°å­—ï¼ˆæ”¯æŒå¸¦é€—å·å’Œç©ºæ ¼çš„æ ¼å¼ï¼‰
        def extract_number(s: str) -> str:
            # æ”¯æŒåƒåˆ†ä½é€—å·: 1,234,567.89
            m = re.search(r'(-?\d{1,3}(?:[,\s]\d{3})*(?:\.\d+)?)', s)
            if m:
                return clean_number(m.group(1))
            # æ™®é€šæ•°å­—
            m = re.search(r'(-?\d+(?:\.\d+)?)', s)
            if m:
                return m.group(1)
            return None
        
        # ğŸ”¥ è¾…åŠ©å‡½æ•°ï¼šæå–æœ€åä¸€ä¸ªæ•°å­—ï¼ˆç”¨äºä» #### å‰æå–ç­”æ¡ˆï¼‰
        def extract_last_number(s: str) -> str:
            # å…ˆå°è¯•åŒ¹é…å¸¦åƒåˆ†ä½çš„å¤§æ•°å­—
            nums = re.findall(r'-?\d{1,3}(?:[,\s]\d{3})+(?:\.\d+)?', s)
            if nums:
                return clean_number(nums[-1])
            # å†å°è¯•æ™®é€šæ•°å­—
            nums = re.findall(r'-?\d+(?:\.\d+)?', s)
            if nums:
                return nums[-1]
            return None
        
        # ç­–ç•¥ 1: æ£€æŸ¥ #### åæ˜¯å¦æœ‰æ•°å­—
        if "####" in text:
            parts = text.split("####")
            if len(parts) > 1:
                after_hash = parts[1].strip()
                # å°è¯•ä» #### åæå–æ•°å­—
                lines = after_hash.split('\n')
                for line in lines:
                    line = line.strip()
                    if not line:
                        continue
                    num = extract_number(line)
                    if num:
                        return num
                    break
                
                # å¦‚æœ #### åæ²¡æœ‰æ•°å­—ï¼Œå¯èƒ½æ˜¯ "ç­”æ¡ˆ\n####" æ ¼å¼
                # ğŸ”¥ ä¿®å¤ï¼šä» #### å‰é¢çš„æ–‡æœ¬ä¸­æå–æœ€åä¸€ä¸ªæ•°å­—
                before_hash = parts[0].strip()
                if before_hash:
                    num = extract_last_number(before_hash)
                    if num:
                        return num
        
        # ç­–ç•¥ 2: æå–æœ€åä¸€ä¸ªæ•°å­—ï¼ˆæ”¯æŒå¸¦é€—å·æ ¼å¼ï¼‰
        nums = re.findall(r'-?\d{1,3}(?:[,\s]\d{3})+(?:\.\d+)?', text)
        if nums:
            return clean_number(nums[-1])
        
        nums = re.findall(r'-?\d+(?:\.\d+)?', text)
        if nums:
            return nums[-1]
        
        return text


class AquaRatHandler(BaseDatasetHandler):
    """AQUA-RAT æ•°æ®é›†å¤„ç†å™¨"""

    def load_and_split(self, test_size: int = 300, seed: int = 42) -> Tuple[Dataset, Dataset]:
        # AQUA-RAT é»˜è®¤ä½¿ç”¨ raw subset
        subset = self.subset or "raw"
        dataset = load_dataset(self.dataset_name, subset)

        # AQUA-RAT æœ‰ train/validation/testï¼Œæˆ‘ä»¬ä½¿ç”¨ train å’Œ test
        train = dataset["train"]
        test = dataset["test"]

        if test_size < len(test):
            np.random.seed(seed)
            test_indices = np.random.choice(len(test), test_size, replace=False).tolist()
            test_indices.sort()
            test = test.select(test_indices)

        return train, test

    def format_example_io(self, example: Dict) -> Tuple[str, str]:
        q = example["question"].strip()
        options = " ".join(example["options"])
        correct = example["correct"].strip()
        return f"{q}\nOptions: {options}", f"The answer is {correct}"

    def format_example_cot(self, example: Dict) -> Tuple[str, str]:
        q = example["question"].strip()
        options = " ".join(example["options"])
        rationale = example["rationale"].strip()
        correct = example["correct"].strip()
        return f"{q}\nOptions: {options}", f"{rationale}\nThe answer is {correct}"

    def extract_gold_answer(self, example: Dict) -> str:
        return example["correct"].strip()

    def extract_prediction(self, model_output: str) -> str:
        # æŸ¥æ‰¾æ¨¡å¼ "The answer is X" æˆ–ç›´æ¥çš„é€‰é¡¹
        import re

        # é¦–å…ˆå°è¯•æ‰¾ GSM8K é£æ ¼çš„ #### X æ ¼å¼ï¼ˆç”¨äº paper æ¨¡å¼ï¼‰
        gsm_match = re.search(r'####\s*([A-E])', model_output, re.IGNORECASE)
        if gsm_match:
            return gsm_match.group(1).upper()

        # æŸ¥æ‰¾ "answer is" æ¨¡å¼ï¼Œå…è®¸åé¢æœ‰æ ‡ç‚¹ç¬¦å·
        answer_pattern = r"(?:the\s+)?answer\s+is\s+([A-E])(?:[.,;:]|\s|$)"
        match = re.search(answer_pattern, model_output, re.IGNORECASE)
        if match:
            return match.group(1).upper()

        # æŸ¥æ‰¾å¸¦å¥å·çš„ç‹¬ç«‹é€‰é¡¹ï¼ˆå¦‚ "D." åœ¨å¥å°¾ï¼‰
        option_with_punct_pattern = r"\b([A-E])\.(?:\s|$)"
        match = re.search(option_with_punct_pattern, model_output)
        if match:
            return match.group(1).upper()

        # æœ€åæŸ¥æ‰¾ç‹¬ç«‹çš„é€‰é¡¹å­—æ¯ï¼ˆä¸å¸¦æ ‡ç‚¹ï¼‰
        option_pattern = r"\b([A-E])\b"
        matches = re.findall(option_pattern, model_output)
        if matches:
            return matches[-1].upper()  # è¿”å›æœ€åä¸€ä¸ªåŒ¹é…çš„é€‰é¡¹

        return model_output.strip()

class Math500Handler(BaseDatasetHandler):
    """Math-500 æ•°æ®é›†å¤„ç†å™¨"""

    def load_and_split(self, test_size: int = 300, seed: int = 42) -> Tuple[Dataset, Dataset]:
        # Math-500 æ•°æ®é›†åŠ è½½ - æ³¨æ„è¿™ä¸ªæ•°æ®é›†åªæœ‰ test split
        dataset = load_dataset(self.dataset_name, split="test")
        
        # è·å–æ•°æ®é›†å¤§å°
        total_size = len(dataset)
        print(f"Math-500 total size: {total_size}")
        
        # å¯¹äº500ä¸ªæ ·æœ¬çš„æ•°æ®é›†ï¼Œè°ƒæ•´åˆ’åˆ†ç­–ç•¥
        # é»˜è®¤æµ‹è¯•é›†300ä¸ªï¼Œè®­ç»ƒé›†200ä¸ª
        if total_size <= 500:
            # ç¡®ä¿æµ‹è¯•é›†ä¸è¶…è¿‡æ€»é‡çš„80%ï¼Œè‡³å°‘ä¿ç•™100ä¸ªè®­ç»ƒæ ·æœ¬
            max_test_size = total_size - 100
            actual_test_size = min(test_size, max_test_size)
            
            # å¦‚æœè¯·æ±‚çš„æµ‹è¯•é›†å¤ªå¤§ï¼Œç»™å‡ºè­¦å‘Š
            if test_size > actual_test_size:
                print(f"Warning: Requested test_size {test_size} is too large for dataset size {total_size}")
                print(f"Adjusted test_size to {actual_test_size} to ensure at least 100 training samples")
        else:
            # å¯¹äºæ›´å¤§çš„æ•°æ®é›†ï¼ˆè™½ç„¶Math-500åº”è¯¥æ­£å¥½æ˜¯500ï¼‰
            actual_test_size = min(test_size, total_size)
            if total_size < test_size + 100:
                actual_test_size = min(test_size, max(50, int(total_size * 0.6)))
                print(f"Adjusted test_size from {test_size} to {actual_test_size} due to dataset size")
        
        # ä½¿ç”¨ç¼“å­˜çš„åˆ’åˆ†
        cache_path = self._get_split_cache_path(actual_test_size, seed)
        cached = self._load_split_indices(cache_path)
        
        if cached:
            train_indices, test_indices = cached
            print(f"Using cached split for Math-500: train={len(train_indices)}, test={len(test_indices)}")
        else:
            # åˆ›å»ºå›ºå®šçš„åˆ’åˆ†
            np.random.seed(seed)
            indices = np.random.permutation(total_size).tolist()
            test_indices = indices[:actual_test_size]
            train_indices = indices[actual_test_size:]
            self._save_split_indices(train_indices, test_indices, cache_path)
            print(f"Created new split for Math-500: train={len(train_indices)}, test={len(test_indices)}")
        
        train = dataset.select(train_indices)
        test = dataset.select(test_indices)
        
        return train, test
    
    def format_example_io(self, example: Dict) -> Tuple[str, str]:
        """æ ¼å¼åŒ–ä¸º I/O æ¨¡å¼çš„ (question, answer)"""
        problem = example["problem"].strip()
        answer = example["answer"].strip()
        
        # æ¸…ç†ç­”æ¡ˆä¸­çš„LaTeXæ ¼å¼ï¼ˆä¿ç•™ä¸»è¦å†…å®¹ï¼‰
        # answer = self._clean_latex_answer(answer)
        
        return problem, answer
    
    def format_example_cot(self, example: Dict) -> Tuple[str, str]:
        """æ ¼å¼åŒ–ä¸º COT æ¨¡å¼çš„ (question, answer_with_reasoning)"""
        problem = example["problem"].strip()
        solution = example["solution"].strip()
        answer = example["answer"].strip()
        
        # æ¸…ç†è§£å†³æ–¹æ¡ˆä¸­çš„å›¾å½¢ä»£ç 
        # solution = self._clean_solution(solution)
        
        # ç»„åˆè§£å†³æ–¹æ¡ˆå’Œæœ€ç»ˆç­”æ¡ˆ
        full_answer = f"{solution}. So the final answer is boxed{{{answer}}}."
        
        return problem, full_answer
    
    def extract_gold_answer(self, example: Dict) -> str:
        """æå–æ ‡å‡†ç­”æ¡ˆ"""
        answer = example["answer"].strip()
        return self._clean_latex_answer(answer).replace(" ", "")
    
    def extract_prediction(self, model_output: str) -> str:
        """ä»æ¨¡å‹è¾“å‡ºä¸­æå–é¢„æµ‹ç­”æ¡ˆ"""
        import re
        
        # ä¼˜å…ˆæŸ¥æ‰¾ \boxed{} æ ¼å¼
        boxed_content = self._extract_boxed_content(model_output)
        if boxed_content:
            return self._clean_latex_answer(boxed_content).replace(" ", "")
        
        # æŸ¥æ‰¾ "answer is" æˆ– "answer:" æ¨¡å¼
        answer_patterns = [
            r"answer\s*is\s*[:\s]*(.+?)(?:\n|$)",
            r"answer\s*[:=]\s*(.+?)(?:\n|$)",
            r"final\s+answer\s*[:=]\s*(.+?)(?:\n|$)",
            r"therefore\s*,?\s*(.+?)(?:\n|$)"
        ]
        
        for pattern in answer_patterns:
            match = re.search(pattern, model_output, re.IGNORECASE)
            if match:
                return self._clean_latex_answer(match.group(1).strip()).replace(" ", "")
        
        # å¦‚æœæ‰¾ä¸åˆ°ç‰¹å®šæ¨¡å¼ï¼Œå°è¯•æå–æœ€åä¸€ä¸ªæ•°å­¦è¡¨è¾¾å¼
        math_expressions = re.findall(r'\$([^$]+)\$', model_output)
        if math_expressions:
            return self._clean_latex_answer(math_expressions[-1]).replace(" ", "")
        
        # æœ€åè¿”å›åŸå§‹è¾“å‡ºçš„æœ€åä¸€è¡Œéç©ºå†…å®¹
        lines = model_output.strip().split('\n')
        for line in reversed(lines):
            if line.strip():
                return self._clean_latex_answer(line.strip()).replace(" ", "")
        
        return model_output.strip().replace(" ", "")
    
    def _extract_boxed_content(self, text: str) -> Optional[str]:
        """æå– \boxed{} ä¸­çš„å†…å®¹ï¼Œæ­£ç¡®å¤„ç†åµŒå¥—çš„èŠ±æ‹¬å·"""
        import re
        
        # æ‰¾åˆ° \boxed{ çš„ä½ç½®
        start_pattern = r"\\boxed\{"
        match = re.search(start_pattern, text)
        if not match:
            return None
        
        # ä» \boxed{ åé¢å¼€å§‹
        start_pos = match.end()
        
        # è®¡æ•°èŠ±æ‹¬å·ï¼Œæ‰¾åˆ°åŒ¹é…çš„å³èŠ±æ‹¬å·
        brace_count = 1
        pos = start_pos
        
        while pos < len(text) and brace_count > 0:
            if text[pos] == '\\' and pos + 1 < len(text):
                # è·³è¿‡è½¬ä¹‰å­—ç¬¦
                pos += 2
                continue
            elif text[pos] == '{':
                brace_count += 1
            elif text[pos] == '}':
                brace_count -= 1
            pos += 1
        
        if brace_count == 0:
            # æ‰¾åˆ°äº†åŒ¹é…çš„å³èŠ±æ‹¬å·
            return text[start_pos:pos - 1]
        else:
            # æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„å³èŠ±æ‹¬å·
            return None
    
    def _clean_latex_answer(self, answer: str) -> str:
        """æ¸…ç†LaTeXæ ¼å¼çš„ç­”æ¡ˆï¼Œä¿ç•™ä¸»è¦å†…å®¹"""
        if not answer:
            return ""
        
        # ç§»é™¤å¤–å±‚çš„ $ ç¬¦å·
        answer = answer.strip()
        if answer.startswith('$') and answer.endswith('$'):
            answer = answer[1:-1]
        
        # ç§»é™¤ \boxed{} åŒ…è£…
        if answer.startswith('\\boxed{') and answer.endswith('}'):
            answer = answer[7:-1]
        
        # åŸºæœ¬çš„LaTeXå‘½ä»¤æ¸…ç†ï¼ˆä¿ç•™ä¸»è¦æ•°å­¦å†…å®¹ï¼‰
        # è¿™é‡Œåªåšæœ€åŸºç¡€çš„æ¸…ç†ï¼Œä¿ç•™å¤§éƒ¨åˆ†LaTeXå‘½ä»¤ä»¥ä¾¿æ­£ç¡®æ¯”è¾ƒ
        answer = answer.strip()
        
        return answer
    
    def _clean_solution(self, solution: str) -> str:
        """æ¸…ç†è§£å†³æ–¹æ¡ˆï¼Œç§»é™¤å›¾å½¢ä»£ç ç­‰"""
        import re
        
        # ç§»é™¤ [asy]...[/asy] ä»£ç å—
        solution = re.sub(r'\[asy\].*?\[/asy\]', '', solution, flags=re.DOTALL)
        
        # ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
        lines = solution.split('\n')
        cleaned_lines = [line for line in lines if line.strip()]
        
        return '\n'.join(cleaned_lines).strip()


class UGPhysicsHandler(BaseDatasetHandler):
    """UGPhysics æ•°æ®é›†å¤„ç†å™¨"""

    # UGPhysics çš„æ‰€æœ‰å­é›†ï¼ˆæŒ‰å¤§å°æ’åºï¼‰
    ALL_SUBSETS = [
        "QuantumMechanics",
        "AtomicPhysics",
        "ClassicalMechanics",
        "StatisticalMechanics",
        "ClassicalElectromagnetism",
        "Thermodynamics",
        "TheoreticalMechanics",
        "WaveOptics",
        "Relativity",
        "SemiconductorPhysics",
        "Electrodynamics",
        "Solid-StatePhysics",
        "GeometricalOptics"
    ]

    def __init__(self, dataset_name: str, subset: str = None, cache_dir: str = "./dataset_splits"):
        super().__init__(dataset_name, subset, cache_dir)
        # UGPhysics å¿…é¡»æŒ‡å®š subset
        if not subset:
            raise ValueError(
                f"UGPhysics requires a subset to be specified. "
                f"Use 'all' or 'mixed' to mix all subsets, or choose from: {', '.join(self.ALL_SUBSETS)}"
            )

    def _save_split_indices_with_size(self, train_indices, test_indices, test_size, cache_path):
        """ä¿å­˜åˆ’åˆ†ç´¢å¼•æ—¶åŒ…å«test_sizeä¿¡æ¯"""
        cache_data = {
            'train_indices': train_indices,
            'test_indices': test_indices,
            'test_size': test_size,
            'train_size': len(train_indices),
            'actual_test_size': len(test_indices)  # ä¿å­˜å®é™…çš„æµ‹è¯•é›†å¤§å°
        }
        os.makedirs(os.path.dirname(cache_path), exist_ok=True)
        with open(cache_path, 'w') as f:
            json.dump(cache_data, f, indent=2)

    def _load_split_indices_with_size(self, cache_path, expected_test_size):
        """åŠ è½½åˆ’åˆ†ç´¢å¼•å¹¶éªŒè¯test_size"""
        if not os.path.exists(cache_path):
            return None

        try:
            with open(cache_path, 'r') as f:
                cache_data = json.load(f)

            # æ£€æŸ¥test_sizeæ˜¯å¦åŒ¹é…
            cached_test_size = cache_data.get('test_size')
            if cached_test_size != expected_test_size:
                print(f"Cache test_size {cached_test_size} doesn't match expected {expected_test_size}")
                return None

            print(f"Found valid cache with test_size={cached_test_size}")
            return cache_data['train_indices'], cache_data['test_indices']
        except Exception as e:
            print(f"Error loading cache: {e}")
            return None

    def load_and_split(self, test_size: int = 300, seed: int = 42) -> Tuple[Dataset, Dataset]:
        """åŠ è½½å¹¶åˆ’åˆ†æ•°æ®é›†"""
        print(f"Requested test_size: {test_size}")

        if self.subset in ["all", "mixed"]:
            return self._load_and_split_mixed(test_size, seed)

        # å•ä¸ªå­é›†çš„å¤„ç†
        dataset = load_dataset(self.dataset_name, self.subset, split="en")
        total_size = len(dataset)
        print(f"UGPhysics/{self.subset} total size: {total_size}")

        # å¦‚æœæ•°æ®é›†å¤ªå°ï¼Œè°ƒæ•´ test_size
        original_test_size = test_size
        if total_size < test_size + 50:  # è‡³å°‘ä¿ç•™50ä¸ªè®­ç»ƒæ ·æœ¬
            test_size = min(test_size, max(20, int(total_size * 0.2)))  # æµ‹è¯•é›†æœ€å¤šå 20%
            print(f"Adjusted test_size from {original_test_size} to {test_size} due to small dataset size")

        # ä½¿ç”¨ç¼“å­˜çš„åˆ’åˆ†ï¼ˆç°åœ¨åŒ…å«test_sizeåœ¨è·¯å¾„ä¸­ï¼‰
        cache_path = self._get_split_cache_path(test_size, seed)
        print(f"Cache path: {cache_path}")

        cached = self._load_split_indices_with_size(cache_path, test_size)

        if cached:
            train_indices, test_indices = cached
            print(f"Using cached split: train={len(train_indices)}, test={len(test_indices)}")
        else:
            print(f"Creating new split with test_size={test_size}")
            # åˆ›å»ºå›ºå®šçš„åˆ’åˆ†
            np.random.seed(seed)
            indices = np.random.permutation(total_size).tolist()
            test_indices = indices[:test_size]
            train_indices = indices[test_size:]
            self._save_split_indices_with_size(train_indices, test_indices, test_size, cache_path)
            print(f"Created new split: train={len(train_indices)}, test={len(test_indices)}")

        train = dataset.select(train_indices)
        test = dataset.select(test_indices)

        print(f"Final split sizes: train={len(train)}, test={len(test)}")

        return train, test

    def _load_and_split_mixed(self, test_size: int = 300, seed: int = 42) -> Tuple[Dataset, Dataset]:
        """åŠ è½½å¹¶æ··åˆæ‰€æœ‰å­é›†"""
        print(f"Loading and mixing all UGPhysics subsets with test_size={test_size}...")

        # ç‰¹æ®Šçš„ç¼“å­˜è·¯å¾„ç”¨äºæ··åˆæ•°æ®é›†ï¼ˆç°åœ¨åŒ…å«test_sizeï¼‰
        cache_path = self._get_mixed_dataset_cache_path(test_size, seed)
        print(f"Mixed dataset cache path: {cache_path}")

        # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„æ··åˆæ•°æ®é›†
        if os.path.exists(cache_path):
            print(f"Found cached mixed dataset")
            with open(cache_path, 'r') as f:
                cache_data = json.load(f)

            # éªŒè¯test_sizeæ˜¯å¦åŒ¹é…
            cached_test_size = cache_data.get('test_size', cache_data.get('actual_test_size', 0))
            if cached_test_size == test_size or len(cache_data.get('test_indices', [])) == test_size:
                print(f"Cache is valid with test_size={cached_test_size}")
                train_indices = cache_data['train_indices']
                test_indices = cache_data['test_indices']
                subset_info = cache_data['subset_info']

                # é‡æ–°åŠ è½½æ•°æ®é›†
                all_datasets = []
                for subset_name, subset_size in subset_info:
                    try:
                        ds = load_dataset(self.dataset_name, subset_name, split="en")
                        # æ·»åŠ å­é›†æ ‡ç­¾
                        ds = ds.add_column("subset", [subset_name] * len(ds))
                        all_datasets.append(ds)
                    except Exception as e:
                        print(f"Warning: Failed to load subset {subset_name}: {e}")

                # åˆå¹¶æ‰€æœ‰æ•°æ®é›†
                mixed_dataset = concatenate_datasets(all_datasets)

                # åº”ç”¨ç¼“å­˜çš„ç´¢å¼•
                train = mixed_dataset.select(train_indices)
                test = mixed_dataset.select(test_indices)

                print(f"Loaded cached split: train={len(train)}, test={len(test)}")

                return train, test
            else:
                print(f"Cache test_size {cached_test_size} doesn't match expected {test_size}, regenerating...")

        # å¦‚æœæ²¡æœ‰ç¼“å­˜æˆ–test_sizeä¸åŒ¹é…ï¼Œåˆ›å»ºæ–°çš„æ··åˆæ•°æ®é›†
        print("Creating new mixed dataset split...")
        all_datasets = []
        subset_info = []

        for subset in self.ALL_SUBSETS:
            try:
                ds = load_dataset(self.dataset_name, subset, split="en")
                size = len(ds)
                print(f"  - {subset}: {size} examples")

                # æ·»åŠ å­é›†æ ‡ç­¾
                ds = ds.add_column("subset", [subset] * size)
                all_datasets.append(ds)
                subset_info.append((subset, size))
            except Exception as e:
                print(f"Warning: Failed to load subset {subset}: {e}")

        # åˆå¹¶æ‰€æœ‰æ•°æ®é›†
        mixed_dataset = concatenate_datasets(all_datasets)
        total_size = len(mixed_dataset)
        print(f"Total mixed dataset size: {total_size} examples")

        # åˆ›å»ºéšæœºåˆ’åˆ†
        np.random.seed(seed)
        indices = np.random.permutation(total_size).tolist()

        # ç¡®ä¿æµ‹è¯•é›†å¤§å°åˆç†
        actual_test_size = test_size
        if total_size < test_size + 100:
            actual_test_size = min(test_size, max(100, int(total_size * 0.1)))
            print(f"Adjusted test_size from {test_size} to {actual_test_size} for mixed dataset")

        test_indices = indices[:actual_test_size]
        train_indices = indices[actual_test_size:]

        # ä¿å­˜æ··åˆæ•°æ®é›†çš„ä¿¡æ¯å’Œç´¢å¼•
        cache_data = {
            'train_indices': train_indices,
            'test_indices': test_indices,
            'subset_info': subset_info,
            'total_size': total_size,
            'test_size': actual_test_size,  # ä¿å­˜å®é™…ä½¿ç”¨çš„test_size
            'train_size': len(train_indices),
            'actual_test_size': len(test_indices),  # å†—ä½™ä½†æ˜ç¡®
            'seed': seed
        }

        os.makedirs(os.path.dirname(cache_path), exist_ok=True)
        with open(cache_path, 'w') as f:
            json.dump(cache_data, f, indent=2)

        train = mixed_dataset.select(train_indices)
        test = mixed_dataset.select(test_indices)

        # æ‰“å°å„å­é›†åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­çš„åˆ†å¸ƒ
        print("\nSubset distribution in train/test:")
        for split_name, split_data in [("Train", train), ("Test", test)]:
            subset_counts = {}
            for example in split_data:
                subset = example['subset']
                subset_counts[subset] = subset_counts.get(subset, 0) + 1

            print(f"\n{split_name} set ({len(split_data)} total):")
            for subset, count in sorted(subset_counts.items(), key=lambda x: x[1], reverse=True):
                print(f"  {subset}: {count} ({count / len(split_data) * 100:.1f}%)")

        return train, test

    def _get_split_cache_path(self, test_size: int, seed: int) -> str:
        """ç”ŸæˆåŒ…å«test_sizeçš„ç¼“å­˜è·¯å¾„"""
        # é‡å†™çˆ¶ç±»æ–¹æ³•ï¼Œç¡®ä¿test_sizeè¢«åŒ…å«åœ¨ç¼“å­˜è·¯å¾„ä¸­
        cache_id = f"{self.dataset_name}_{self.subset}_{test_size}_{seed}"
        cache_hash = hashlib.md5(cache_id.encode()).hexdigest()[:8]
        return os.path.join(self.cache_dir, f"split_{cache_hash}.json")

    def _get_mixed_dataset_cache_path(self, test_size: int, seed: int) -> str:
        """ç”Ÿæˆæ··åˆæ•°æ®é›†çš„ç¼“å­˜è·¯å¾„ï¼ˆåŒ…å«test_sizeï¼‰"""
        cache_id = f"{self.dataset_name}_mixed_all_{test_size}_{seed}"
        cache_hash = hashlib.md5(cache_id.encode()).hexdigest()[:8]
        return os.path.join(self.cache_dir, f"mixed_{cache_hash}.json")

    def format_example_io(self, example: Dict) -> Tuple[str, str]:
        """æ ¼å¼åŒ–è¾“å…¥è¾“å‡ºç¤ºä¾‹"""
        problem = example["problem"].strip()
        answer = example["answers"].strip()
        unit = example.get("unit", "").strip()

        if unit and unit != "null" and unit != "None":
            return problem, f"{answer} {unit}"
        return problem, answer

    def format_example_cot(self, example: Dict) -> Tuple[str, str]:
        """æ ¼å¼åŒ–é“¾å¼æ€ç»´ç¤ºä¾‹"""
        problem = example["problem"].strip()
        solution = example["solution"].strip()
        answer = example["answers"].strip()

        # æ£€æŸ¥ unit æ˜¯å¦å­˜åœ¨ä¸”ä¸ºå­—ç¬¦ä¸²ç±»å‹
        unit = example.get("unit")
        if isinstance(unit, str):
            unit = unit.strip()
        else:
            unit = None

        if unit and unit != "null" and unit != "None":
            full_answer = f"{solution}\nFinal answer: {answer} {unit}"
        else:
            full_answer = f"{solution}\nFinal answer: {answer}"

        return problem, full_answer

    def extract_boxed_content(self, text):
        """æå– \boxed{} ä¸­çš„å†…å®¹ï¼Œæ­£ç¡®å¤„ç†åµŒå¥—çš„èŠ±æ‹¬å·"""
        import re

        # æ‰¾åˆ° \boxed{ çš„ä½ç½®
        start_pattern = r"\\boxed\{"
        match = re.search(start_pattern, text)
        if not match:
            return None

        # ä» \boxed{ åé¢å¼€å§‹
        start_pos = match.end()

        # è®¡æ•°èŠ±æ‹¬å·ï¼Œæ‰¾åˆ°åŒ¹é…çš„å³èŠ±æ‹¬å·
        brace_count = 1
        pos = start_pos

        while pos < len(text) and brace_count > 0:
            if text[pos] == '{':
                brace_count += 1
            elif text[pos] == '}':
                brace_count -= 1
            pos += 1

        if brace_count == 0:
            # æ‰¾åˆ°äº†åŒ¹é…çš„å³èŠ±æ‹¬å·
            return text[start_pos:pos - 1]
        else:
            # æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„å³èŠ±æ‹¬å·
            return None

    def extract_gold_answer(self, example: Dict) -> str:
        """æå–æ ‡å‡†ç­”æ¡ˆ"""
        answer = example["answers"].strip()
        boxed_content = self.extract_boxed_content(answer)
        if boxed_content:
            return boxed_content
        return answer

    def extract_prediction(self, model_output: str) -> str:
        """ä»æ¨¡å‹è¾“å‡ºä¸­æå–é¢„æµ‹ç­”æ¡ˆ"""
        import re

        # æŸ¥æ‰¾ boxed æ ¼å¼
        boxed_content = self.extract_boxed_content(model_output)
        if boxed_content:
            return boxed_content

        # æŸ¥æ‰¾ "Final answer:" æ¨¡å¼
        final_pattern = r"Final answer:\s*(.+?)(?:\n|$)"
        match = re.search(final_pattern, model_output, re.IGNORECASE)
        if match:
            return match.group(1).strip()

        # å°è¯•æå–æœ€åçš„æ•°å€¼
        from metrics_utils import parse_number_from_text
        num = parse_number_from_text(model_output)
        if num is not None:
            return str(num)

        return model_output.strip()

class SVAMPHandler(BaseDatasetHandler):
    """SVAMP æ•°æ®é›†å¤„ç†å™¨"""

    def load_and_split(self, test_size: int = 300, seed: int = 42) -> Tuple[Dataset, Dataset]:
        # SVAMP æ•°æ®é›†æœ‰ train å’Œ test splits
        train_dataset = load_dataset(self.dataset_name, split="train")
        test_dataset = load_dataset(self.dataset_name, split="test")
        
        # è·å–æ•°æ®é›†å¤§å°
        train_size = len(train_dataset)
        test_size_original = len(test_dataset)
        print(f"SVAMP train size: {train_size}, test size: {test_size_original}")
        
        # å¦‚æœæµ‹è¯•é›†å·²ç»è¶³å¤Ÿå°ï¼Œç›´æ¥ä½¿ç”¨
        if test_size_original <= test_size:
            print(f"Using full test set with {test_size_original} samples")
            return train_dataset, test_dataset
        
        # å¦åˆ™ï¼Œä»æµ‹è¯•é›†ä¸­é‡‡æ ·
        # ä½¿ç”¨ç¼“å­˜çš„åˆ’åˆ†
        cache_path = self._get_split_cache_path(test_size, seed)
        cached = self._load_split_indices(cache_path)
        
        if cached:
            _, test_indices = cached
            print(f"Using cached test subset: {len(test_indices)} samples from {test_size_original}")
            test_subset = test_dataset.select(test_indices)
        else:
            # åˆ›å»ºå›ºå®šçš„æµ‹è¯•å­é›†
            np.random.seed(seed)
            test_indices = np.random.permutation(test_size_original).tolist()[:test_size]
            # ä¿å­˜æ—¶ï¼Œtrain_indices ä¼ å…¥ç©ºåˆ—è¡¨ï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨å®Œæ•´çš„è®­ç»ƒé›†
            self._save_split_indices([], test_indices, cache_path)
            print(f"Created test subset: {len(test_indices)} samples from {test_size_original}")
            test_subset = test_dataset.select(test_indices)
        
        return train_dataset, test_subset
    
    def format_example_io(self, example: Dict) -> Tuple[str, str]:
        """æ ¼å¼åŒ–ä¸º I/O æ¨¡å¼çš„ (question, answer)"""
        question = example["question_concat"].strip()
        answer = str(example["Answer"]).strip()
        
        return question, answer
    
    def format_example_cot(self, example: Dict) -> Tuple[str, str]:
        """SVAMP æ²¡æœ‰ COT ç‰ˆæœ¬ï¼Œè¿”å›ä¸ IO ç›¸åŒçš„æ ¼å¼"""
        return self.format_example_io(example)
    
    def extract_gold_answer(self, example: Dict) -> str:
        """æå–æ ‡å‡†ç­”æ¡ˆ"""
        return str(example["Answer"]).strip()
    
    def extract_prediction(self, model_output: str) -> str:
        """ä»æ¨¡å‹è¾“å‡ºä¸­æå–é¢„æµ‹ç­”æ¡ˆ"""
        import re
        
        # ç­–ç•¥1: æŸ¥æ‰¾ #### æ ¼å¼ï¼ˆç±»ä¼¼GSM8Kï¼‰
        gsm_match = re.search(r'####\s*(\S+)', model_output)
        if gsm_match:
            return gsm_match.group(1).strip()
        
        # ç­–ç•¥2: æŸ¥æ‰¾ "answer is" æˆ– "answer:" æ¨¡å¼
        answer_patterns = [
            r"answer\s*is\s*[:\s]*(\d+(?:\.\d+)?)",
            r"answer\s*[:=]\s*(\d+(?:\.\d+)?)",
            r"=\s*(\d+(?:\.\d+)?)\s*$",  # ç­‰å·åé¢çš„æ•°å­—ï¼ˆåœ¨è¡Œæœ«ï¼‰
            r"(?:total|result|sum)\s*(?:is|=)\s*(\d+(?:\.\d+)?)",
            r"therefore\s*,?\s*(\d+(?:\.\d+)?)",
        ]
        
        for pattern in answer_patterns:
            match = re.search(pattern, model_output, re.IGNORECASE | re.MULTILINE)
            if match:
                return match.group(1).strip()
        
        # ç­–ç•¥3: æå–æœ€åä¸€ä¸ªç‹¬ç«‹çš„æ•°å­—
        # æŸ¥æ‰¾æ‰€æœ‰çš„æ•°å­—
        numbers = re.findall(r'\b\d+(?:\.\d+)?\b', model_output)
        if numbers:
            # è¿”å›æœ€åä¸€ä¸ªæ•°å­—
            return numbers[-1]
        
        # ç­–ç•¥4: æŸ¥æ‰¾æœ€åä¸€è¡Œçš„å†…å®¹
        lines = model_output.strip().split('\n')
        for line in reversed(lines):
            line = line.strip()
            if line:
                # å°è¯•ä»æœ€åä¸€è¡Œæå–æ•°å­—
                num_match = re.search(r'\d+(?:\.\d+)?', line)
                if num_match:
                    return num_match.group()
        
        # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œè¿”å›åŸå§‹è¾“å‡º
        return model_output.strip()

class ModelScopeCOTHandler(BaseDatasetHandler):
    def _hf_from_msdataset(self, ms_ds):
        data_list = []
        for item in ms_ds:
            data_list.append(dict(item))
        return Dataset.from_list(data_list)

    def _filter_by_tasks(self, ds_list, task_list):
        if not task_list:
            return ds_list
        task_set = set(task_list)
        return [ex for ex in ds_list if str(ex.get("task", "")).strip() in task_set]

    def load_and_split(self, test_size: int = 300, seed: int = 42) -> Tuple[Dataset, Dataset]:
        # ç›´æ¥ä»æœ¬åœ°åŠ è½½æ•°æ®é›†
        local_path = "/data/oujie/models/AI-ModelScope/CoT-Collection"
        print(f"ä»æœ¬åœ°åŠ è½½æ•°æ®é›†: {local_path}")
        
        try:
            # ä½¿ç”¨ HuggingFace datasets ä»æœ¬åœ°è·¯å¾„åŠ è½½
            from datasets import load_dataset as hf_load_dataset
            hf_dataset = hf_load_dataset(
                local_path,
                "en",
                split="train",
                trust_remote_code=True
            )
            train_ms = hf_dataset
            print(f"æˆåŠŸåŠ è½½æ•°æ®é›†ï¼Œæ€»æ•°æ®é‡: {len(train_ms)}")
        except Exception as e:
            print(f"æœ¬åœ°åŠ è½½å¤±è´¥: {e}")
            raise RuntimeError(f"æ— æ³•ä»æœ¬åœ°è·¯å¾„åŠ è½½æ•°æ®é›†: {local_path}")
        
        full_list = [dict(x) for x in train_ms]
        task_list = getattr(self, "task_list", None)
        print(f"task list:{task_list}")
        full_list = self._filter_by_tasks(full_list, task_list)
        total_size = len(full_list)
        actual_test_size = min(test_size, total_size)
        tasks = sorted(set([str(t).strip() for t in (task_list or []) if str(t).strip()]))
        task_sig = hashlib.md5(",".join(tasks).encode()).hexdigest()[:8] if tasks else "none"
        cache_id = f"{self.dataset_name}_{self.subset or 'default'}_{actual_test_size}_{seed}_{task_sig}"
        cache_hash = hashlib.md5(cache_id.encode()).hexdigest()[:8]
        cache_path = os.path.join(self.cache_dir, f"split_{cache_hash}.json")
        cached = self._load_split_indices(cache_path)
        if cached:
            train_indices, test_indices = cached
            max_idx = max([max(train_indices) if train_indices else -1, max(test_indices) if test_indices else -1])
            if max_idx >= total_size:
                train_indices, test_indices = None, None
        else:
            np.random.seed(seed)
            indices = np.random.permutation(total_size).tolist()
            test_indices = indices[:actual_test_size]
            train_indices = indices[actual_test_size:]
            os.makedirs(os.path.dirname(cache_path), exist_ok=True)
            with open(cache_path, 'w') as f:
                json.dump({
                    'train_indices': train_indices,
                    'test_indices': test_indices,
                    'total_size': total_size,
                    'task_sig': task_sig,
                    'dataset_name': self.dataset_name,
                    'subset': self.subset or 'default',
                    'seed': seed,
                    'test_size': actual_test_size
                }, f, indent=2)
        train_list = [full_list[i] for i in train_indices]
        test_list = [full_list[i] for i in test_indices]
        return Dataset.from_list(train_list), Dataset.from_list(test_list)

    def format_example_io(self, example: Dict) -> Tuple[str, str]:
        q = str(example.get("source", "")).strip()
        a = str(example.get("target", "")).strip()
        return q, a

    def format_example_cot(self, example: Dict) -> Tuple[str, str]:
        q = str(example.get("source", "")).strip()
        r = str(example.get("rationale", "")).strip()
        t = str(example.get("target", "")).strip()
        return q, f"{r}\nFinal answer: {t}"

    def extract_gold_answer(self, example: Dict) -> str:
        return str(example.get("target", "")).strip()

    def extract_prediction(self, model_output: str) -> str:
        """
        ä»COT-Collectionæ•°æ®é›†çš„æ¨¡å‹è¾“å‡ºä¸­æå–é¢„æµ‹ç­”æ¡ˆ
        
        æ”¯æŒçš„ç­”æ¡ˆæ ¼å¼:
        - Final answer: xxx
        - é€‰æ‹©é¢˜: a, b, c, d, A, B, C, D
        - æ˜¯/å¦é¢˜: Yes, No, a, b
        - æ•°å­—ç­”æ¡ˆ
        """
        import re
        
        # ğŸ”¥ æ­¥éª¤ 0: é¢„å¤„ç† - æˆªæ–­åˆ°ç¬¬ä¸€ä¸ª "Final answer" åçš„ç­”æ¡ˆï¼Œé¿å…æ¨¡å‹ç»§ç»­ç”Ÿæˆæ–°é—®é¢˜
        first_final_match = re.search(r'Final\s+answer\s*:\s*(.+?)(?:\n|$)', model_output, re.IGNORECASE)
        if first_final_match:
            answer_text = first_final_match.group(1).strip()
            
            # æ¸…ç†ç­”æ¡ˆæ–‡æœ¬ï¼šç§»é™¤å°¾éšçš„æ— å…³å†…å®¹
            noise_patterns = [
                r'You are an AI',
                r'Problem:',
                r'Solution:',
                r'Question:',
                r'Context:',
                r'\n\n',
            ]
            for pattern in noise_patterns:
                noise_match = re.search(pattern, answer_text, re.IGNORECASE)
                if noise_match:
                    answer_text = answer_text[:noise_match.start()].strip()
            
            return self._extract_core_answer(answer_text)
        
        # æ­¥éª¤ 1: å°è¯•åŒ¹é… "answer is xxx" æ ¼å¼
        m = re.search(r'answer\s+is\s+(.+?)(?:[\n\r.]|$)', model_output, re.IGNORECASE)
        if m:
            return self._extract_core_answer(m.group(1).strip())
        
        # æ­¥éª¤ 2: å°è¯•åŒ¹é…ç‹¬ç«‹çš„é€‰æ‹©é¡¹ç­”æ¡ˆ
        lines = model_output.strip().split('\n')
        for line in lines:
            line = line.strip()
            option_match = re.match(r'^[`\(]?\s*([abcdABCD])\s*[\)`]?[\s\.\)]?(?:for\s+)?(Yes|No)?[\s\.]*$', line, re.IGNORECASE)
            if option_match:
                return option_match.group(1).lower()
            
            choice_match = re.match(r'^([abcdABCD])\s*[\)\.]?\s*(for\s+)?(Yes|No)?\.?\s*$', line, re.IGNORECASE)
            if choice_match:
                return choice_match.group(1).lower()
        
        # æ­¥éª¤ 3: å¦‚æœä»¥ä¸Šéƒ½å¤±è´¥ï¼Œè¿”å›ç¬¬ä¸€è¡Œéç©ºå†…å®¹
        for line in lines:
            line = line.strip()
            if line and not line.startswith('Problem:') and not line.startswith('You are'):
                return self._extract_core_answer(line)
        
        return model_output.strip()[:100]
    
    def _extract_core_answer(self, answer_text: str) -> str:
        """ä»ç­”æ¡ˆæ–‡æœ¬ä¸­æå–æ ¸å¿ƒç­”æ¡ˆ"""
        import re
        
        answer_text = answer_text.strip()
        answer_text = re.sub(r'[\.,;:!?]+$', '', answer_text).strip()
        
        # åŒ¹é…é€‰æ‹©é¡¹æ ¼å¼
        option_match = re.match(r'^[`\(]?\s*([abcdABCD1234])\s*[\)`]?(?:\s*[\)\.]\s*(?:for\s+)?(?:Yes|No|yes|no)?)?\s*[`]?$', answer_text, re.IGNORECASE)
        if option_match:
            opt = option_match.group(1)
            if opt in '1234':
                return chr(ord('a') + int(opt) - 1)
            return opt.lower()
        
        # åŒ¹é… "a. Western Bulldogs" æ ¼å¼
        option_with_text = re.match(r'^([abcdABCD])\s*[\)\.:]\s*', answer_text)
        if option_with_text:
            return option_with_text.group(1).lower()
        
        # åŒ¹é…æ•°å­—ç­”æ¡ˆ
        num_match = re.match(r'^(\d+(?:[,\s]\d{3})*(?:\.\d+)?)\s*$', answer_text)
        if num_match:
            return num_match.group(1).replace(',', '').replace(' ', '')
        
        # åŒ¹é… Yes/No
        if answer_text.lower() in ['yes', 'no']:
            return answer_text.lower()
        
        # æˆªæ–­è¿‡é•¿çš„ç­”æ¡ˆ
        if len(answer_text) > 200:
            first_line = answer_text.split('\n')[0].strip()
            first_sentence = re.split(r'[.!?]', answer_text)[0].strip()
            return first_line if len(first_line) <= 100 else first_sentence[:100]
        
        return answer_text

DATASET_HANDLERS = {
    "openai/gsm8k": GSM8KHandler,
    "deepmind/aqua_rat": AquaRatHandler,
    "UGPhysics/ugphysics": UGPhysicsHandler,
    "HuggingFaceH4/MATH-500": Math500Handler,  # Math-500 æ•°æ®é›†
    "ChilleD/SVAMP": SVAMPHandler,  # SVAMP æ•°æ®é›†
    "AI-ModelScope/CoT-Collection": ModelScopeCOTHandler,
}


def get_dataset_handler(dataset_name: str, subset: str = None, tasks: str = None) -> BaseDatasetHandler:
    """è·å–å¯¹åº”çš„æ•°æ®é›†å¤„ç†å™¨
    
    Args:
        dataset_name: æ•°æ®é›†åç§°
        subset: æ•°æ®é›†å­é›†
        tasks: ä»»åŠ¡åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰ï¼Œç”¨äºè¿‡æ»¤ ModelScope CoT-Collection ç­‰æ•°æ®é›†
    """
    if dataset_name not in DATASET_HANDLERS:
        raise ValueError(f"Unsupported dataset: {dataset_name}")

    handler_class = DATASET_HANDLERS[dataset_name]
    handler = handler_class(dataset_name, subset)
    
    # å¦‚æœæä¾›äº† tasks å‚æ•°ï¼Œä¸” handler æ”¯æŒ task_listï¼Œåˆ™è®¾ç½®å®ƒ
    if tasks and hasattr(handler, '_filter_by_tasks'):
        task_list = [t.strip() for t in tasks.split(',') if t.strip()]
        handler.task_list = task_list
    
    return handler


def list_available_datasets_and_subsets():
    """åˆ—å‡ºæ‰€æœ‰æ”¯æŒçš„æ•°æ®é›†å’Œå®ƒä»¬çš„å­é›†"""
    print("Available datasets and their subsets:")
    for dataset_name in DATASET_HANDLERS:
        print(f"\n{dataset_name}:")
        try:
            subsets = BaseDatasetHandler.get_available_subsets(dataset_name)
            if subsets:
                for subset in subsets:
                    print(f"  - {subset}")
            else:
                print("  - (no subsets found)")
        except Exception as e:
            print(f"  - Error: {e}")


# å¦‚æœç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶ï¼Œæ˜¾ç¤ºå¯ç”¨çš„æ•°æ®é›†å’Œå­é›†
if __name__ == "__main__":
    list_available_datasets_and_subsets()