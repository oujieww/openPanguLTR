"""
æ¢é’ˆé€‰æ‹©å™¨
æŒ‰çª—å£å¤§å° n é€è½®æ‰©å±• shotsï¼Œé€šè¿‡ç†µåˆ¤æ–­ç½®ä¿¡åº¦

æµç¨‹:
1. æŒ‰ ranked_shots é¡ºåºï¼Œæ¯è½®å¼•å…¥ n ä¸ªæ–° shot
2. ä½¿ç”¨ KV cache æ„é€  probe è¾“å…¥: concat(K_shots, K_prompt_query)
3. ç”Ÿæˆ 1 ä¸ª tokenï¼Œè®¡ç®—è¾“å‡ºåˆ†å¸ƒçš„ç†µ
4. å¦‚æœç†µ < é˜ˆå€¼ï¼Œè®¤ä¸ºå½“å‰ shots è¶³å¤Ÿï¼Œåœæ­¢æ‰©å±•
5. å¦åˆ™ç»§ç»­ä¸‹ä¸€è½®
"""
import sys
import os
import torch
import logging
import numpy as np
from typing import List, Tuple, Dict
from tqdm.auto import tqdm

# æ·»åŠ  util è·¯å¾„ï¼ˆå¤åˆ¶è‡ª baseline çš„é€šç”¨æ¨¡å—ï¼‰
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'util'))

from kv_pool_manager import KVPoolManager


class ProbeSelector:
    """æ¢é’ˆé€‰æ‹©å™¨"""
    
    # ğŸ”¥ æ¢é’ˆé—®é¢˜ï¼šè®©æ¨¡å‹å›ç­” Yes/Noï¼Œé™åˆ¶è¾“å‡ºåœ¨å• token
    PROBE_QUESTION = "Based on the above examples, are you confident to answer this question? Please answer Yes or No."
    
    def __init__(
        self,
        model,
        tokenizer,
        kv_pool_manager: KVPoolManager,
        window_size: int = 4,
        entropy_threshold: float = 0.5,
        max_rounds: int = 256,
        device: str = "npu",
        verbose: bool = True,
        mode: str = "cot",  # æ·»åŠ æ¨¡å¼å‚æ•°
        paper_num_questions: int = 4,  # Paper æ¨¡å¼çš„ question-only shots æ•°é‡
        output_dir: str = None  # è¾“å‡ºç›®å½•
    ):
        """
        åˆå§‹åŒ–æ¢é’ˆé€‰æ‹©å™¨
        
        Args:
            model: è¯­è¨€æ¨¡å‹
            tokenizer: åˆ†è¯å™¨
            kv_pool_manager: KV æ± ç®¡ç†å™¨
            window_size: æ¯è½®å¼•å…¥çš„ shot æ•°é‡ (n)
            entropy_threshold: ç†µé˜ˆå€¼ (Ï„)
            max_rounds: æœ€å¤§æ¢é’ˆè½®æ•°
            device: è®¡ç®—è®¾å¤‡
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—
            mode: æ¨¡å¼ ('cot', 'io', 'paper')
            paper_num_questions: Paper æ¨¡å¼çš„ question-only shots æ•°é‡
            output_dir: è¾“å‡ºç›®å½•ï¼ˆç”¨äºä¿å­˜æ¢é’ˆ prompt ç¤ºä¾‹ï¼‰
        """
        self.model = model
        self.tokenizer = tokenizer
        self.kv_pool = kv_pool_manager
        self.window_size = window_size
        self.entropy_threshold = entropy_threshold
        self.max_rounds = max_rounds
        self.device = device
        self.verbose = verbose
        self.mode = mode
        self.paper_num_questions = paper_num_questions
        self.output_dir = output_dir
        
        # ğŸ”¥ æ ‡å¿—ä½ï¼šåªä¿å­˜ä¸€æ¬¡æ¢é’ˆ prompt ç¤ºä¾‹
        self._probe_example_saved = False
        
        # æ¨¡å‹é…ç½®
        self.num_layers = model.config.num_hidden_layers
        self.num_heads = model.config.num_attention_heads
        self.num_key_value_heads = getattr(model.config, 'num_key_value_heads', model.config.num_attention_heads)
        self.head_dim = model.config.hidden_size // model.config.num_attention_heads
        
        # ğŸ”¥ é¢„ç¼–è¯‘ Yes/No token IDs
        self._prepare_yesno_tokens()
        
        logging.info(f"åˆå§‹åŒ– ProbeSelector: window_size={window_size}, "
                    f"entropy_threshold={entropy_threshold}, max_rounds={max_rounds}, mode={mode}")
    
    def _prepare_yesno_tokens(self):
        """é¢„ç¼–è¯‘ Yes/No çš„ token IDs"""
        yes_variants = ["Yes", "yes", "YES", " Yes", " yes"]
        no_variants = ["No", "no", "NO", " No", " no"]
        
        self.yes_token_ids = set()
        self.no_token_ids = set()
        
        for word in yes_variants:
            tokens = self.tokenizer.encode(word, add_special_tokens=False)
            self.yes_token_ids.update(tokens)
        
        for word in no_variants:
            tokens = self.tokenizer.encode(word, add_special_tokens=False)
            self.no_token_ids.update(tokens)
        
        self.yesno_token_ids = list(self.yes_token_ids.union(self.no_token_ids))
        
        if self.verbose:
            logging.info(f"Yes tokens: {self.yes_token_ids}, No tokens: {self.no_token_ids}")
    
    def _get_prompt_query_kv(self, prompt_tokens: torch.Tensor, query_tokens: torch.Tensor):
        """è·å– prompt+query çš„ KV"""
        input_ids = torch.cat([prompt_tokens, query_tokens], dim=-1)
        if input_ids.dim() == 1:
            input_ids = input_ids.unsqueeze(0)
        input_ids = input_ids.to(self.device)
        
        with torch.inference_mode():
            outputs = self.model(
                input_ids=input_ids,
                use_cache=True,
                return_dict=True
            )
            K_layers, V_layers = self.kv_pool._extract_kv_from_model_output(outputs)
        
        return K_layers, V_layers
    
    def _reshape_kv_to_past_format(self, K_layers, V_layers):
        """
        å°†ç®€åŒ–çš„ KV è½¬æ¢ä¸º past_key_values æ ¼å¼
        
        Args:
            K_layers: List[K_layer], æ¯ä¸ª K_layer: [T, d_k]
            V_layers: List[V_layer], æ¯ä¸ª V_layer: [T, d_v]
        
        Returns:
            past_key_values: tuple of (key, value) pairs
        """
        past_key_values = []
        
        for layer_idx in range(len(K_layers)):
            K = K_layers[layer_idx]  # [T, d_k]
            V = V_layers[layer_idx]  # [T, d_v]
            
            seq_len = K.shape[0]
            assert K.shape[1] == self.num_key_value_heads * self.head_dim, (
                f"K dim mismatch at layer {layer_idx}: {K.shape[1]} vs {self.num_key_value_heads * self.head_dim}")
            assert V.shape[1] == self.num_key_value_heads * self.head_dim, (
                f"V dim mismatch at layer {layer_idx}: {V.shape[1]} vs {self.num_key_value_heads * self.head_dim}")
            assert K.shape[0] == V.shape[0], (
                f"Seq len mismatch at layer {layer_idx}: K={K.shape[0]} V={V.shape[0]}")
            K = K.contiguous()
            V = V.contiguous()
            
            # Reshape: [T, num_key_value_heads * head_dim] -> [1, num_key_value_heads, T, head_dim]
            K_reshaped = K.view(seq_len, self.num_key_value_heads, self.head_dim).unsqueeze(0)
            K_reshaped = K_reshaped.permute(0, 2, 1, 3)  # [1, num_key_value_heads, T, head_dim]
            
            V_reshaped = V.view(seq_len, self.num_key_value_heads, self.head_dim).unsqueeze(0)
            V_reshaped = V_reshaped.permute(0, 2, 1, 3)
            
            past_key_values.append((K_reshaped, V_reshaped))
        
        return tuple(past_key_values)
    
    def _compute_entropy(self, logits: torch.Tensor) -> float:
        """
        è®¡ç®— Yes/No è¾“å‡ºåˆ†å¸ƒçš„ç†µ
        
        Args:
            logits: è¾“å‡º logits [vocab_size]
        
        Returns:
            entropy: ç†µå€¼
        """
        # æ£€æŸ¥ logits æ˜¯å¦æœ‰æ•ˆ
        if torch.isnan(logits).any() or torch.isinf(logits).any():
            logging.warning("âš ï¸ logits åŒ…å« NaN æˆ– Infï¼Œè¿”å›é»˜è®¤é«˜ç†µå€¼")
            return 10.0
        
        # ğŸ”¥ åªå…³æ³¨ Yes/No ç›¸å…³çš„ token
        if len(self.yesno_token_ids) > 0:
            yesno_logits = logits[self.yesno_token_ids]
            probs = torch.softmax(yesno_logits.float(), dim=-1)
        else:
            # fallback: ä½¿ç”¨å…¨è¯æ±‡è¡¨
            probs = torch.softmax(logits.float(), dim=-1)
        
        # è®¡ç®—ç†µ H = -sum(p * log(p))
        log_probs = torch.log(probs + 1e-10)
        entropy = -torch.sum(probs * log_probs).item()
        
        # æœ€åæ£€æŸ¥ä¸€æ¬¡
        if not torch.isfinite(torch.tensor(entropy)):
            logging.warning(f"âš ï¸ ç†µå€¼è®¡ç®—ç»“æœå¼‚å¸¸: {entropy}ï¼Œè¿”å›é»˜è®¤é«˜ç†µå€¼")
            return 10.0
        
        return entropy
    
    def _get_kv_excluding_last_token(self, text: str) -> Tuple[List[torch.Tensor], List[torch.Tensor], int]:
        """
        ä¸ºæ–‡æœ¬è®¡ç®— KV cacheï¼Œä½†æ’é™¤æœ€åä¸€ä¸ª token
        è¿”å› KV cache å’Œæœ€åä¸€ä¸ª token çš„ ID
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
        
        Returns:
            (K_layers, V_layers, last_token_id): ä¸å«æœ€åä¸€ä¸ª token çš„ KV å’Œæœ€åä¸€ä¸ª token ID
        """
        # Tokenize
        tokens = self.tokenizer.encode(text, add_special_tokens=False, return_tensors='pt')[0]
        
        if len(tokens) <= 1:
            # å¦‚æœåªæœ‰ä¸€ä¸ª tokenï¼Œè¿”å›ç©º KV å’Œè¯¥ token
            last_token_id = tokens[-1].item() if len(tokens) > 0 else self.tokenizer.eos_token_id
            empty_K = [torch.zeros((0, self.kv_pool.d_k), device=self.device) for _ in range(self.num_layers)]
            empty_V = [torch.zeros((0, self.kv_pool.d_v), device=self.device) for _ in range(self.num_layers)]
            return empty_K, empty_V, last_token_id
        
        # ä¿å­˜æœ€åä¸€ä¸ª token ID
        last_token_id = tokens[-1].item()
        
        # ä¸ºä¸å«æœ€åä¸€ä¸ª token çš„åºåˆ—è®¡ç®— KV
        tokens_without_last = tokens[:-1]
        
        with torch.inference_mode():
            input_ids = tokens_without_last.unsqueeze(0).to(self.device)
            outputs = self.model(
                input_ids=input_ids,
                use_cache=True,
                return_dict=True
            )
            K_layers, V_layers = self.kv_pool._extract_kv_from_model_output(outputs)
        
        return K_layers, V_layers, last_token_id
    
    def _run_probe(
        self,
        prompt_tokens: torch.Tensor,
        candidate_shot_ids: List[int],
        query_tokens: torch.Tensor,
        query_text: str = "",  # æ·»åŠ  query æ–‡æœ¬å‚æ•°ç”¨äºæ„å»º prompt
        query_kv_cache: Tuple = None,  # é¢„è®¡ç®—çš„ query KV cacheï¼ˆä¸å«æœ€åä¸€ä¸ª tokenï¼‰
        start_token_id: int = None  # é¢„è®¡ç®—çš„èµ·å§‹ token ID
    ) -> Tuple[float, str]:
        """
        è¿è¡Œä¸€æ¬¡æ¢é’ˆï¼ˆä½¿ç”¨ KV cacheï¼Œæ”¯æŒä¸åŒæ¨¡å¼ï¼‰
        
        Args:
            prompt_tokens: Prompt token IDs
            candidate_shot_ids: å€™é€‰ shot ID åˆ—è¡¨
            query_tokens: Query token IDs
            query_text: Query æ–‡æœ¬ï¼ˆç”¨äºæ„å»ºæ ¼å¼åŒ– promptï¼‰
            query_kv_cache: é¢„è®¡ç®—çš„ query KV cacheï¼ˆä¸å«æœ€åä¸€ä¸ª tokenï¼‰
            start_token_id: èµ·å§‹ token IDï¼ˆquery çš„æœ€åä¸€ä¸ª tokenï¼‰
        
        Returns:
            (entropy, response): ç†µå€¼å’Œç”Ÿæˆçš„ token æ–‡æœ¬
        """
        # âœ… æ­¥éª¤ 0: è·å–å›ºå®šéƒ¨åˆ†çš„ KVï¼ˆSystem Prompt æˆ– Paper å›ºå®šéƒ¨åˆ†ï¼‰
        if self.mode == "paper":
            # Paper æ¨¡å¼ï¼šä½¿ç”¨é¢„ç¼“å­˜çš„å›ºå®šéƒ¨åˆ†ï¼ˆå¼•å¯¼è¯­ + fullshotsï¼‰
            K_fixed_layers, V_fixed_layers = self.kv_pool.get_fixed_kv('paper_fixed')
        else:
            # CoT/IO æ¨¡å¼ï¼šä½¿ç”¨ System Prompt
            K_fixed_layers, V_fixed_layers = self.kv_pool.get_fixed_kv('system_prompt')
        
        # âœ… æ­¥éª¤ 1: æ ¹æ®æ¨¡å¼è·å– shots çš„ KV
        if self.mode == "paper":
            # Paper æ¨¡å¼ï¼šåªéœ€è¦ question-only shots
            # æ³¨æ„ï¼špaper çš„ fullshots å·²ç»åœ¨å›ºå®šéƒ¨åˆ†äº†
            K_shots_layers, V_shots_layers = self._get_paper_question_only_kv(candidate_shot_ids)
        else:
            # CoT/IO æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨é¢„è®¡ç®—çš„ KV
            K_shots_layers, V_shots_layers = self.kv_pool.get_kv_for_shots(candidate_shot_ids)
        
        # 2. è·å– query + æ¢é’ˆé—®é¢˜ çš„ KVï¼ˆä½¿ç”¨é¢„è®¡ç®—çš„ç¼“å­˜æˆ–ç°åœºè®¡ç®—ï¼‰
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šquery KV ä¸åŒ…å«æœ€åä¸€ä¸ª tokenï¼Œæœ€åä¸€ä¸ª token ä½œä¸º start_token è¾“å…¥
        if query_kv_cache is not None:
            # ä½¿ç”¨é¢„è®¡ç®—çš„ query KV cacheï¼ˆå·²æ’é™¤æœ€åä¸€ä¸ª tokenï¼‰
            K_query_layers, V_query_layers = query_kv_cache
        elif query_text:
            # ğŸ”¥ Fallback: ä½¿ç”¨æ¢é’ˆé—®é¢˜æ ¼å¼
            formatted_query = f"Problem: {query_text}\n{self.PROBE_QUESTION}"
            K_query_layers, V_query_layers, start_token_id = self._get_kv_excluding_last_token(formatted_query)
        else:
            # ä½¿ç”¨åŸå§‹ tokens
            K_query_layers, V_query_layers = self._get_prompt_query_kv(prompt_tokens, query_tokens)
        
        # âœ… æ­¥éª¤ 3: æ‹¼æ¥æ‰€æœ‰éƒ¨åˆ†ï¼šå›ºå®šéƒ¨åˆ† + shots + query
        K_all_layers = []
        V_all_layers = []
        for layer_idx in range(self.num_layers):
            parts_K = []
            parts_V = []
            
            # 1. å›ºå®šéƒ¨åˆ†
            if K_fixed_layers[layer_idx].shape[0] > 0:
                parts_K.append(K_fixed_layers[layer_idx])
                parts_V.append(V_fixed_layers[layer_idx])
            
            # 2. Shots
            if K_shots_layers[layer_idx].shape[0] > 0:
                parts_K.append(K_shots_layers[layer_idx])
                parts_V.append(V_shots_layers[layer_idx])
            
            # 3. Query
            if K_query_layers[layer_idx].shape[0] > 0:
                parts_K.append(K_query_layers[layer_idx])
                parts_V.append(V_query_layers[layer_idx])
            
            # æ‹¼æ¥
            K_all = torch.cat(parts_K, dim=0) if parts_K else torch.zeros((0, self.kv_pool.d_k), device=self.device)
            V_all = torch.cat(parts_V, dim=0) if parts_V else torch.zeros((0, self.kv_pool.d_v), device=self.device)
            
            K_all_layers.append(K_all)
            V_all_layers.append(V_all)
        
        # 4. è½¬æ¢ä¸º past_key_values æ ¼å¼
        print(f"[DEBUG-PROBE] å¼€å§‹è½¬æ¢ past_key_values, å±‚æ•°={len(K_all_layers)}, seq_len={K_all_layers[0].shape[0] if K_all_layers else 0}")
        past_key_values = self._reshape_kv_to_past_format(K_all_layers, V_all_layers)
        print(f"[DEBUG-PROBE] past_key_values è½¬æ¢å®Œæˆ, shape={past_key_values[0][0].shape}")
        
        # 5. ç”Ÿæˆ 1 ä¸ªtoken - ä½¿ç”¨æ‰‹åŠ¨ forward è€Œä¸æ˜¯ generate
        # ğŸ”¥ ä½¿ç”¨é¢„è®¡ç®—çš„ start_token_idï¼Œé¿å…é‡å¤
        if start_token_id is not None:
            start_token = torch.tensor([[start_token_id]], dtype=torch.long, device=self.device)
        else:
            # fallback: ä½¿ç”¨ query çš„æœ€åä¸€ä¸ª token
            start_token = query_tokens[-1:].long().unsqueeze(0).to(self.device)
        
        # è®¡ç®— past_key_values çš„é•¿åº¦ï¼ˆå·²ç»ç¼“å­˜çš„ tokens æ•°é‡ï¼‰
        past_length = K_all_layers[0].shape[0] if K_all_layers else 0
        
        # åˆ›å»º attention_mask: æ‰€æœ‰ past tokens + å½“å‰ input token éƒ½æ˜¯æœ‰æ•ˆçš„
        attention_mask = torch.ones((1, past_length + 1), dtype=torch.long, device=self.device)
        
        with torch.inference_mode():
            # æ‰‹åŠ¨ forward è·å– logits
            print(f"[DEBUG-PROBE] å‡†å¤‡ forward, start_token shape={start_token.shape}, attention_mask shape={attention_mask.shape}")
            outputs = self.model(
                input_ids=start_token,
                attention_mask=attention_mask,
                past_key_values=past_key_values,
                use_cache=True,
                return_dict=True
            )
            print(f"[DEBUG-PROBE] forward å®Œæˆ")
            
            # è·å–æœ€åä¸€ä¸ªä½ç½®çš„ logits
            first_token_logits = outputs.logits[0, -1, :]  # [vocab_size]
            
            # è®¡ç®—ç†µ
            entropy = self._compute_entropy(first_token_logits)
            
            # ğŸ”¥ åœ¨ Yes/No tokens ä¸­é€‰æ‹©æœ€å¯èƒ½çš„ tokenï¼ˆè€Œä¸æ˜¯å…¨è¯æ±‡è¡¨ï¼‰
            if len(self.yesno_token_ids) > 0:
                yesno_logits = first_token_logits[self.yesno_token_ids]
                best_idx = torch.argmax(yesno_logits).item()
                next_token_id = self.yesno_token_ids[best_idx]
            else:
                next_token_id = torch.argmax(first_token_logits).item()
            
            response = self.tokenizer.decode([next_token_id], skip_special_tokens=False)
            
            # è¾“å‡ºè°ƒè¯•ä¿¡æ¯
            print(f"[DEBUG-PROBE] next_token_id={next_token_id}, response='{response}'")
        
        return entropy, response
    
    def _get_paper_question_only_kv(self, shot_ids: List[int]):
        """
        ä¸º Paper æ¨¡å¼è·å– question-only shots çš„ KV
        
        Args:
            shot_ids: Shot ID åˆ—è¡¨
        
        Returns:
            (K_layers, V_layers): æ‹¼è£…å¥½çš„ KV
        """
        # æ„å»º question-onlyéƒ¨åˆ†
        parts = []
        if shot_ids:
            parts.append("You will be provided Problems similar to the ones below:")
            for sid in shot_ids:
                if sid in self.kv_pool.kv_cache_pool:
                    example = self.kv_pool.kv_cache_pool[sid]['example']
                    q, _ = self.kv_pool.dataset_handler.format_example_cot(example)
                    parts.append(f"Problem: {q}")
            parts.append("â€”")  # åˆ†éš”ç¬¦
        
        if parts:
            combined_text = "\n".join(parts)
            K_layers, V_layers = self.kv_pool.get_kv_for_text(combined_text)
        else:
            # è¿”å›ç©º KV
            K_layers = [torch.zeros((0, self.kv_pool.d_k), device=self.device) for _ in range(self.num_layers)]
            V_layers = [torch.zeros((0, self.kv_pool.d_v), device=self.device) for _ in range(self.num_layers)]
        
        return K_layers, V_layers
    
    def _save_probe_prompt_example(self, query_text: str, shot_ids: List[int]):
        """
        ä¿å­˜å®Œæ•´çš„æ¢é’ˆ prompt ç¤ºä¾‹æ–‡ä»¶ä¾›å®¡æŸ¥ï¼ˆåªä¿å­˜ä¸€æ¬¡ï¼‰
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            shot_ids: Shot ID åˆ—è¡¨
        """
        # ğŸ”¥ åªä¿å­˜ä¸€æ¬¡
        if self._probe_example_saved:
            return
        
        try:
            import os
            from datetime import datetime
            
            # æ„å»ºå®Œæ•´çš„ prompt
            parts = []
            
            # 1. System Prompt
            system_prompt = self.kv_pool.get_fixed_text('system_prompt')
            if system_prompt:
                parts.append("=" * 60)
                parts.append("[SYSTEM PROMPT]")
                parts.append("=" * 60)
                parts.append(system_prompt)
            
            # 2. Shots
            parts.append("\n" + "=" * 60)
            parts.append(f"[SHOTS ({len(shot_ids)} ä¸ª)]")
            parts.append("=" * 60)
            for i, sid in enumerate(shot_ids):
                if sid in self.kv_pool.kv_cache_pool:
                    example = self.kv_pool.kv_cache_pool[sid]['example']
                    q, a = self.kv_pool.dataset_handler.format_example_cot(example)
                    parts.append(f"\n--- Shot {i+1} (ID={sid}) ---")
                    parts.append(f"Problem: {q}")
                    parts.append(f"Solution: {a}")
            
            # 3. Query + æ¢é’ˆé—®é¢˜
            parts.append("\n" + "=" * 60)
            parts.append("[QUERY + æ¢é’ˆé—®é¢˜]")
            parts.append("=" * 60)
            parts.append(f"Problem: {query_text}")
            parts.append(f"\n{self.PROBE_QUESTION}")
            
            # ğŸ”¥ ä½¿ç”¨é…ç½®çš„è¾“å‡ºç›®å½•ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨è„šæœ¬ç›®å½•
            if self.output_dir:
                save_dir = self.output_dir
            else:
                save_dir = os.path.dirname(os.path.abspath(__file__))
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(save_dir, exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filepath = os.path.join(save_dir, f"probe_prompt_example_{timestamp}.txt")
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write("\n".join(parts))
            
            # ğŸ”¥ æ ‡è®°å·²ä¿å­˜
            self._probe_example_saved = True
            logging.info(f"ğŸ“„ å®Œæ•´æ¢é’ˆ prompt å·²ä¿å­˜åˆ°: {filepath}")
        except Exception as e:
            logging.warning(f"âš ï¸ ä¿å­˜æ¢é’ˆ prompt å¤±è´¥: {e}")
    
    def select_shots_with_probe(
        self,
        ranked_shots: List[int],
        prompt_tokens: torch.Tensor,
        query_tokens: torch.Tensor,
        query_text: str = ""  # æ·»åŠ  query æ–‡æœ¬å‚æ•°
    ) -> Tuple[List[int], List[Dict]]:
        """
        ä½¿ç”¨æ¢é’ˆæœºåˆ¶é€‰æ‹© shots
        
        Args:
            ranked_shots: æ’åºåçš„ shot ID åˆ—è¡¨
            prompt_tokens: Prompt token IDs
            query_tokens: Query token IDs
            query_text: Query æ–‡æœ¬ï¼ˆç”¨äºæ ¼å¼åŒ–ï¼‰
        
        Returns:
            (selected_shots, probe_history): é€‰ä¸­çš„ shot åˆ—è¡¨å’Œæ¢é’ˆå†å²
        """
        selected_shots = []
        probe_history = []
        
        idx = 0
        round_num = 0
        
        # ğŸ”¥ ä¼˜åŒ–:é¢„è®¡ç®— query + æ¢é’ˆé—®é¢˜ çš„ KV(ä¸å«æœ€åä¸€ä¸ª token),é¿å…æ¯è½®é‡å¤è®¡ç®—
        query_kv_cache = None
        start_token_id = None  # ä¿å­˜æœ€åä¸€ä¸ª token ID
        if query_text and self.mode in ["cot", "io", "paper"]:
            if self.verbose:
                logging.info(f"ğŸ“Œ é¢„è®¡ç®— query + æ¢é’ˆé—®é¢˜ KV cache...")
            try:
                # ğŸ”¥ æ„å»ºåŒ…å«æ¢é’ˆé—®é¢˜çš„å®Œæ•´ query
                # æ¨¡å‹çœ‹åˆ°çš„å†…å®¹ï¼š[System Prompt] + [Shots] + [Query + æ¢é’ˆé—®é¢˜]
                formatted_query = f"Problem: {query_text}\n{self.PROBE_QUESTION}"
                
                # è·å–ä¸å«æœ€åä¸€ä¸ª token çš„ KV
                K_layers, V_layers, start_token_id = self._get_kv_excluding_last_token(formatted_query)
                query_kv_cache = (K_layers, V_layers)
                
                # é‡æ–° tokenize ä»¥è·å–æ­£ç¡®çš„ query_tokens
                query_tokens = self.tokenizer.encode(formatted_query, add_special_tokens=False, return_tensors='pt')[0]
                if self.verbose:
                    logging.info(f"Query+æ¢é’ˆé—®é¢˜ tokens é•¿åº¦: {len(query_tokens)}, KV é•¿åº¦: {K_layers[0].shape[0]}")
                    logging.info(f"âœ“ Query KV cache é¢„è®¡ç®—å®Œæˆ, start_token_id={start_token_id} ('{self.tokenizer.decode([start_token_id])}')") 
            except Exception as e:
                logging.warning(f"âš ï¸ Query KV é¢„è®¡ç®—å¤±è´¥,å°†åœ¨æ¯è½®é‡æ–°è®¡ç®—: {e}")
                import traceback
                traceback.print_exc()
                query_kv_cache = None
                start_token_id = None
        
        if self.verbose:
            logging.info(f"å¼€å§‹æ¢é’ˆé€‰æ‹©ï¼ˆ{self.mode} æ¨¡å¼ï¼‰ï¼Œæ€»å…± {len(ranked_shots)} ä¸ªå€™é€‰ shot...")
        
        # æ³¨æ„ï¼šæ¢é’ˆ prompt ç¤ºä¾‹åœ¨æ¢é’ˆå®Œæˆåä¿å­˜ï¼Œä»¥å±•ç¤ºæœ€ç»ˆé€‰å®šçš„ shots
        
        while idx < len(ranked_shots) and round_num < self.max_rounds:
            round_num += 1
            
            # æœ¬è½®æ–°å¼•å…¥çš„ shots
            new_shots = ranked_shots[idx : idx + self.window_size]
            candidate_shots = selected_shots + new_shots
            
            # è¿è¡Œæ¢é’ˆï¼Œä¼ å…¥é¢„è®¡ç®—çš„ query KV å’Œ start_token_id
            print(f"[DEBUG-PROBE] è½® {round_num}: å‡†å¤‡è¿è¡Œæ¢é’ˆ, candidate_shots={len(candidate_shots)}")
            entropy, response = self._run_probe(
                prompt_tokens, candidate_shots, query_tokens, query_text, query_kv_cache, start_token_id
            )
            print(f"[DEBUG-PROBE] è½® {round_num}: æ¢é’ˆå®Œæˆ, entropy={entropy:.4f}")
            
            # è®°å½•å†å²
            probe_record = {
                'round': round_num,
                'num_shots': len(candidate_shots),
                'new_shots': new_shots,
                'entropy': entropy,
                'response': response,
                'threshold': self.entropy_threshold,
                'meets_threshold': entropy < self.entropy_threshold,
                'mode': self.mode
            }
            probe_history.append(probe_record)
            
            if self.verbose:
                logging.info(f"æ¢é’ˆè½® {round_num}: shots={len(candidate_shots)}, "
                           f"entropy={entropy:.4f}, threshold={self.entropy_threshold}, "
                           f"response='{response[:20]}'")
            
            # åˆ¤æ–­æ˜¯å¦æ»¡è¶³é˜ˆå€¼
            if entropy < self.entropy_threshold:
                selected_shots = candidate_shots
                if self.verbose:
                    logging.info(f"âœ“ æ¢é’ˆè½® {round_num} æ»¡è¶³é˜ˆå€¼ï¼Œæœ€ç»ˆé€‰ä¸­ {len(selected_shots)} ä¸ª shot")
                break
            else:
                # ä¸å¤Ÿå¥½ï¼Œä½†ä»ç„¶åŠ å…¥å€™é€‰
                selected_shots = candidate_shots
                idx += self.window_size
        
        # å¦‚æœè·‘å®Œæ‰€æœ‰è½®è¿˜æ²¡è¾¾åˆ°é˜ˆå€¼
        if round_num >= self.max_rounds or idx >= len(ranked_shots):
            if self.verbose:
                logging.info(f"è¾¾åˆ°æœ€å¤§è½®æ•°æˆ–ç”¨å°½æ‰€æœ‰ shotï¼Œæœ€ç»ˆé€‰ä¸­ {len(selected_shots)} ä¸ª shot")
        
        # ğŸ”¥ æ¢é’ˆå®Œæˆåï¼Œä¿å­˜æœ€ç»ˆé€‰å®šçš„ shots å¯¹åº”çš„ prompt ç¤ºä¾‹
        self._save_probe_prompt_example(query_text, selected_shots)
        
        return selected_shots, probe_history
