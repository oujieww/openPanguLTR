"""KV æ‹¼è£…å™¨
è´Ÿè´£å°†é€‰ä¸­ shots çš„ KV cache ä¸ prompt+query çš„ KV æ‹¼è£…æˆæœ€ç»ˆåºåˆ—

é€»è¾‘åºåˆ—:
[prompt_tokens, shot_a, shot_b, ..., shot_k, prompt_tokens, query_tokens]

KV æ‹¼è£…:
K_final_layer = concat(K_shots_layer, K_prompt_query_layer, dim=0) ä¸ºæ¯ä¸€å±‚
V_final_layer = concat(V_shots_layer, V_prompt_query_layer, dim=0)

æ”¯æŒä¸‰ç§æ¨¡å¼: CoT, IO, Paper

æ”¯æŒä¸¤ç§ KV å¤ç”¨ç­–ç•¥:
1. æ–‡æœ¬æ‹¼æ¥æ¨¡å¼ (use_text_forward=True): å®‰å…¨ä½†æ…¢
2. KV å¤ç”¨æ¨¡å¼ (use_kv_reuse=True): å¿«é€Ÿï¼Œä½¿ç”¨ RoPE ä½ç½®æ ¡æ­£
"""
import sys
import os
import torch
import logging
import copy
from typing import List, Tuple, Optional, Dict

# æ·»åŠ æ ¹è·¯å¾„ä»¥å®šä½ util åŒ…
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from kv_pool_manager import KVPoolManager
from rope_utils import RoPECorrector  # ğŸ”¥ æ–°å¢: RoPE ä½ç½®æ ¡æ­£å™¨


class KVAssembler:
    """KV æ‹¼è£…å™¨"""
    
    def __init__(
        self,
        model,
        tokenizer,
        kv_pool_manager: KVPoolManager,
        device: str = "npu",
        mode: str = "cot",
        paper_num_questions: int = 4,
        use_text_forward: bool = False,  # æ–‡æœ¬æ‹¼æ¥æ¨¡å¼ï¼ˆå®‰å…¨ä½†æ…¢ï¼‰
        use_kv_reuse: bool = True  # ğŸ”¥ KV å¤ç”¨æ¨¡å¼ï¼ˆå¿«é€Ÿï¼Œä½¿ç”¨ RoPE æ ¡æ­£ï¼‰
    ):
        """
        åˆå§‹åŒ– KV æ‹¼è£…å™¨
        
        Args:
            model: è¯­è¨€æ¨¡å‹
            tokenizer: åˆ†è¯å™¨
            kv_pool_manager: KV æ± ç®¡ç†å™¨
            device: è®¡ç®—è®¾å¤‡
            mode: æ¨¡å¼ ('cot', 'io', 'paper')
            paper_num_questions: Paper æ¨¡å¼çš„ question-only shots æ•°é‡
            use_text_forward: æ–‡æœ¬æ‹¼æ¥æ¨¡å¼ï¼ˆæ¯æ¬¡é‡æ–°è®¡ç®—æ‰€æœ‰ KVï¼Œå®‰å…¨ä½†æ…¢ï¼‰
            use_kv_reuse: KV å¤ç”¨æ¨¡å¼ï¼ˆå¤ç”¨é¢„è®¡ç®—çš„ KV + RoPE ä½ç½®æ ¡æ­£ï¼Œå¿«é€Ÿï¼‰
        """
        self.model = model
        self.tokenizer = tokenizer
        self.kv_pool = kv_pool_manager
        self.device = device
        self.mode = mode
        self.paper_num_questions = paper_num_questions
        self.use_text_forward = use_text_forward
        self.use_kv_reuse = use_kv_reuse  # ğŸ”¥ KV å¤ç”¨æ¨¡å¼
        self.use_hybrid_mode = False  # ç¦ç”¨æ—§æ¨¡å¼
        self._last_part_sizes = {"fixed": 0, "shots": 0, "query": 0}
        
        # æ¨¡å‹é…ç½®
        self.num_layers = model.config.num_hidden_layers
        self.num_heads = model.config.num_attention_heads
        self.num_key_value_heads = getattr(model.config, 'num_key_value_heads', model.config.num_attention_heads)
        self.head_dim = model.config.hidden_size // model.config.num_attention_heads
        
        # ğŸ”¥ åˆå§‹åŒ– RoPE æ ¡æ­£å™¨
        if use_kv_reuse:
            self.rope_corrector = RoPECorrector(model, device)
            logging.info("âœ… RoPE æ ¡æ­£å™¨å·²åˆå§‹åŒ–ï¼Œå¯ç”¨ KV å¤ç”¨æ¨¡å¼")
        else:
            self.rope_corrector = None
        
        logging.info(f"åˆå§‹åŒ– KVAssembler: num_layers={self.num_layers}, "
                    f"num_heads={self.num_heads}, head_dim={self.head_dim}, mode={mode}")
    
    def _get_layer_device(self, layer_idx: int):
        """
        è·å–æŒ‡å®šå±‚çš„è®¾å¤‡
        æ”¯æŒæ¨¡å‹åˆ†ç‰‡ï¼ˆdevice_map='auto'ï¼‰
        
        Args:
            layer_idx: å±‚ç´¢å¼•
        
        Returns:
            device: è¯¥å±‚æ‰€åœ¨çš„è®¾å¤‡
        """
        # è·å–è¯¥å±‚çš„è®¾å¤‡
        layer_module = self.model.model.layers[layer_idx]
        # ä»å±‚çš„ç¬¬ä¸€ä¸ªå‚æ•°è·å–è®¾å¤‡
        return next(layer_module.parameters()).device
    
    def _get_prompt_query_kv(
        self,
        prompt_tokens: torch.Tensor,
        query_tokens: torch.Tensor
    ) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:
        """
        è·å– prompt + query çš„ KV
        
        Args:
            prompt_tokens: Prompt token IDs
            query_tokens: Query token IDs
        
        Returns:
            (K_layers, V_layers): æ‰€æœ‰å±‚çš„ KV
        """
        # æ‹¼æ¥ prompt å’Œ query
        input_ids = torch.cat([prompt_tokens, query_tokens], dim=-1).contiguous().clone()
        if input_ids.dim() == 1:
            input_ids = input_ids.unsqueeze(0)
        # ç¡®ä¿æ˜¯æ•´æ•°ç±»å‹ï¼ˆNPUè¦æ±‚ï¼‰
        input_ids = input_ids.long().to(self.device)
        
        with torch.inference_mode():
            outputs = self.model(
                input_ids=input_ids,
                use_cache=True,
                return_dict=True
            )
            
            # æå– KV (æ‰€æœ‰å±‚)
            K_layers, V_layers = self.kv_pool._extract_kv_from_model_output(outputs)
        
        return K_layers, V_layers
    
    def _reshape_kv_to_past_format(
        self,
        K_layers: List[torch.Tensor],
        V_layers: List[torch.Tensor]
    ) -> Tuple:
        """
        å°†ç®€åŒ–çš„ KV è½¬æ¢ä¸º past_key_values æ ¼å¼
        
        past_key_values æ ¼å¼: tuple of num_layers
        æ¯å±‚: (key, value)
        key/value shape: [batch, num_heads, seq_len, head_dim]
        
        Args:
            K_layers: List[K_layer], æ¯ä¸ª K_layer: [T, d_k]
            V_layers: List[V_layer], æ¯ä¸ª V_layer: [T, d_v]
        
        Returns:
            past_key_values: tuple of (key, value) pairs
        """
        past_key_values = []
        
        for layer_idx in range(len(K_layers)):
            K = K_layers[layer_idx]  # [T, d_k]
            V = V_layers[layer_idx]  # [T, d_v]
            
            seq_len = K.shape[0]
            assert K.shape[1] == self.num_key_value_heads * self.head_dim, (
                f"K dim mismatch at layer {layer_idx}: {K.shape[1]} vs {self.num_key_value_heads * self.head_dim}")
            assert V.shape[1] == self.num_key_value_heads * self.head_dim, (
                f"V dim mismatch at layer {layer_idx}: {V.shape[1]} vs {self.num_key_value_heads * self.head_dim}")
            assert K.shape[0] == V.shape[0], (
                f"Seq len mismatch at layer {layer_idx}: K={K.shape[0]} V={V.shape[0]}")
            K = K.contiguous()
            V = V.contiguous()
            
            # Reshape: [T, num_key_value_heads * head_dim] -> [1, num_key_value_heads, T, head_dim]
            K_reshaped = K.view(seq_len, self.num_key_value_heads, self.head_dim).unsqueeze(0)
            K_reshaped = K_reshaped.permute(0, 2, 1, 3)  # [1, num_key_value_heads, T, head_dim]
            
            V_reshaped = V.view(seq_len, self.num_key_value_heads, self.head_dim).unsqueeze(0)
            V_reshaped = V_reshaped.permute(0, 2, 1, 3)  # [1, num_key_value_heads, T, head_dim]
            
            past_key_values.append((K_reshaped, V_reshaped))
        
        return tuple(past_key_values)
    
    def assemble_kv_for_generation(
        self,
        selected_shot_ids: List[int],
        prompt_tokens: torch.Tensor,
        query_tokens: torch.Tensor,
        query_text: str = ""  # æ·»åŠ  query æ–‡æœ¬å‚æ•°
    ) -> Tuple:
        """
        æ‹¼è£…æœ€ç»ˆçš„ KV cache ç”¨äºç”Ÿæˆï¼ˆæ”¯æŒä¸åŒæ¨¡å¼ï¼‰
        
        Args:
            selected_shot_ids: é€‰ä¸­çš„ shot ID åˆ—è¡¨
            prompt_tokens: Prompt token IDs
            query_tokens: Query token IDs
            query_text: Query æ–‡æœ¬ï¼ˆç”¨äºæ ¼å¼åŒ–ï¼‰
        
        Returns:
            past_key_values: æ‹¼è£…å¥½çš„ KV cache
        """
        logging.info(f"å¼€å§‹æ‹¼è£… KV cacheï¼ˆ{self.mode} æ¨¡å¼ï¼‰ï¼Œå…± {len(selected_shot_ids)} ä¸ª shot...")
        print(f"[DEBUG-KV] å¼€å§‹æ‹¼è£…, mode={self.mode}, shots={len(selected_shot_ids)}")
        
        # âœ… æ­¥éª¤ 0: è·å–å›ºå®šéƒ¨åˆ†çš„ KVï¼ˆSystem Prompt æˆ– Paper å›ºå®šéƒ¨åˆ†ï¼‰
        if self.mode == "paper":
            # Paper æ¨¡å¼ï¼šä½¿ç”¨é¢„ç¼“å­˜çš„å›ºå®šéƒ¨åˆ†ï¼ˆå¼•å¯¼è¯­ + fullshotsï¼‰
            K_fixed_layers, V_fixed_layers = self.kv_pool.get_fixed_kv('paper_fixed')
            logging.info(f"âœ… ä½¿ç”¨ Paper å›ºå®šéƒ¨åˆ† KVï¼Œtokens: {K_fixed_layers[0].shape[0] if K_fixed_layers[0].shape[0] > 0 else 0}")
        else:
            # CoT/IO æ¨¡å¼ï¼šä½¿ç”¨ System Prompt
            K_fixed_layers, V_fixed_layers = self.kv_pool.get_fixed_kv('system_prompt')
            logging.info(f"âœ… ä½¿ç”¨ System Prompt KVï¼Œtokens: {K_fixed_layers[0].shape[0] if K_fixed_layers[0].shape[0] > 0 else 0}")
        
        # âœ… æ­¥éª¤ 1: è·å–åŠ¨æ€é€‰ä¸­çš„ shots KV
        if self.mode == "paper":
            # Paper æ¨¡å¼ï¼šåªéœ€è¦ question-only shots
            # æ³¨æ„ï¼špaper çš„ fullshots å·²ç»åœ¨å›ºå®šéƒ¨åˆ†äº†ï¼Œè¿™é‡Œåªéœ€è¦é¢å¤–çš„ question-only
            K_shots_layers, V_shots_layers = self._get_paper_question_only_kv(selected_shot_ids)
        else:
            # CoT/IO æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨é¢„è®¡ç®—çš„ KV
            K_shots_layers, V_shots_layers = self.kv_pool.get_kv_for_shots(selected_shot_ids)
            logging.info(f"âœ… ç›´æ¥ä½¿ç”¨é¢„è®¡ç®—çš„ KV cacheï¼Œæ— éœ€é‡æ–°è®¡ç®—")
        
        logging.info(f"Shots KV: {len(K_shots_layers)} å±‚, æ¯å±‚ K shape={K_shots_layers[0].shape}")
        
        # âœ… æ­¥éª¤ 2: è·å– query çš„ KVï¼ˆæ ¹æ®æ¨¡å¼æ ¼å¼åŒ–ï¼‰
        if query_text and self.mode in ["cot", "io"]:
            header = "When you respond, your last line must be exactly of the form '#### <final_answer>'."
            if self.mode == "cot":
                formatted_query = f"{header}\nProblem: {query_text}\nSolution:"
            else:
                formatted_query = f"{header}\nProblem: {query_text}\nAnswer:"
            K_query_layers, V_query_layers = self.kv_pool.get_kv_for_text(formatted_query)
        elif self.mode == "paper" and query_text:
            header = "When you respond, your last line must be exactly of the form '#### <final_answer>'."
            formatted_query = f"{header}\nProblem: {query_text}\nSolution:"
            K_query_layers, V_query_layers = self.kv_pool.get_kv_for_text(formatted_query)
        else:
            # ä½¿ç”¨åŸå§‹ tokens
            K_query_layers, V_query_layers = self._get_prompt_query_kv(prompt_tokens, query_tokens)
        
        logging.info(f"Query KV: {len(K_query_layers)} å±‚, æ¯å±‚ K shape={K_query_layers[0].shape}")

        # è®°å½•å„éƒ¨åˆ† token æ•°ï¼ˆä»¥ç¬¬ 0 å±‚ä¸ºå‡†ï¼‰
        try:
            self._last_part_sizes = {
                "fixed": int(K_fixed_layers[0].shape[0]),
                "shots": int(K_shots_layers[0].shape[0]),
                "query": int(K_query_layers[0].shape[0])
            }
        except Exception:
            self._last_part_sizes = {"fixed": 0, "shots": 0, "query": 0}
        
        # âœ… æ­¥éª¤ 3: æ‹¼æ¥æ‰€æœ‰éƒ¨åˆ†ï¼šå›ºå®šéƒ¨åˆ† + shots + query
        # ğŸ”¥ NPU ä¼˜åŒ–ï¼šå…ˆç§»è‡³ CPU æ‹¼æ¥ï¼Œå†ä¼ å›å¯¹åº”å±‚çš„ NPUï¼Œé¿å… ConcatD å†…å­˜é”™è¯¯
        K_all_layers = []
        V_all_layers = []
        for layer_idx in range(self.num_layers):
            parts_K = []
            parts_V = []
            
            # 1. å›ºå®šéƒ¨åˆ† (System Prompt æˆ– Paper å›ºå®šéƒ¨åˆ†)
            if K_fixed_layers[layer_idx].shape[0] > 0:
                parts_K.append(K_fixed_layers[layer_idx].cpu())  # ç§»è‡³ CPU
                parts_V.append(V_fixed_layers[layer_idx].cpu())
            
            # 2. Shots
            if K_shots_layers[layer_idx].shape[0] > 0:
                parts_K.append(K_shots_layers[layer_idx].cpu())  # ç§»è‡³ CPU
                parts_V.append(V_shots_layers[layer_idx].cpu())
            
            # 3. Query
            if K_query_layers[layer_idx].shape[0] > 0:
                parts_K.append(K_query_layers[layer_idx].cpu())  # ç§»è‡³ CPU
                parts_V.append(V_query_layers[layer_idx].cpu())
            
            # è·å–è¯¥å±‚çš„å®é™…è®¾å¤‡ï¼ˆæ”¯æŒåˆ†ç‰‡æ¨¡å‹ï¼‰
            layer_device = self._get_layer_device(layer_idx)
            
            # åœ¨ CPU ä¸Šæ‹¼æ¥ï¼ˆé¿å… NPU ConcatD é”™è¯¯ï¼‰
            K_final = torch.cat(parts_K, dim=0).contiguous().clone()  # CPU ä¸Šæ‹¼æ¥
            K_final = K_final.to(layer_device)  # ä¼ å›è¯¥å±‚çš„ NPU
            
            V_final = torch.cat(parts_V, dim=0).contiguous().clone()  # CPU ä¸Šæ‹¼æ¥
            V_final = V_final.to(layer_device)  # ä¼ å›è¯¥å±‚çš„ NPU
            
            K_all_layers.append(K_final)
            V_all_layers.append(V_final)
        
        logging.info(f"æœ€ç»ˆ KV: {len(K_all_layers)} å±‚, æ¯å±‚ K shape={K_all_layers[0].shape}")
        
        # 4. è½¬æ¢ä¸º past_key_values æ ¼å¼
        past_key_values = self._reshape_kv_to_past_format(K_all_layers, V_all_layers)
        print("[KV] KV cache æ‹¼æ¥å®Œæˆ:", past_key_values[0][0].shape)
        return past_key_values
    
    def generate_with_kv_cache(
        self,
        selected_shot_ids: List[int],
        prompt_tokens: torch.Tensor,
        query_tokens: torch.Tensor,
        max_new_tokens: int = 512,
        query_text: str = "",
        **gen_kwargs
    ) -> Tuple[str, Dict]:
        """
        ä½¿ç”¨æ‹¼è£…çš„ KV cache ç”Ÿæˆç­”æ¡ˆ
        
        Args:
            selected_shot_ids: é€‰ä¸­çš„ shot ID åˆ—è¡¨
            prompt_tokens: Prompt token IDs
            query_tokens: Query token IDs
            max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            query_text: Query æ–‡æœ¬ï¼ˆç”¨äºæ ¼å¼åŒ–ï¼‰
            **gen_kwargs: å…¶ä»–ç”Ÿæˆå‚æ•°
        
        Returns:
            (response, gen_info): ç”Ÿæˆçš„æ–‡æœ¬å’Œç”Ÿæˆä¿¡æ¯
        """
        # ğŸ”¥ KV å¤ç”¨æ¨¡å¼ï¼ˆå¤ç”¨é¢„è®¡ç®—çš„ KV + RoPE ä½ç½®æ ¡æ­£ï¼‰
        if self.use_kv_reuse and self.rope_corrector is not None:
            return self._generate_with_kv_reuse(
                selected_shot_ids, prompt_tokens, query_tokens,
                max_new_tokens, query_text, **gen_kwargs
            )
        
        # æ–‡æœ¬æ‹¼æ¥æ¨¡å¼ï¼ˆå®‰å…¨ä½†æ…¢ï¼‰
        if self.use_text_forward:
            return self._generate_with_text_forward(
                selected_shot_ids, prompt_tokens, query_tokens,
                max_new_tokens, query_text, **gen_kwargs
            )
        
        # æ—§çš„æ··åˆæ¨¡å¼ï¼ˆå·²ç¦ç”¨ï¼‰
        if self.use_hybrid_mode:
            return self._generate_with_hybrid_mode(
                selected_shot_ids, prompt_tokens, query_tokens,
                max_new_tokens, query_text, **gen_kwargs
            )
        
        # é»˜è®¤ï¼šæ–‡æœ¬æ‹¼æ¥æ¨¡å¼
        return self._generate_with_text_forward(
            selected_shot_ids, prompt_tokens, query_tokens,
            max_new_tokens, query_text, **gen_kwargs
        )
    
    def _generate_with_text_forward(
        self,
        selected_shot_ids: List[int],
        prompt_tokens: torch.Tensor,
        query_tokens: torch.Tensor,
        max_new_tokens: int = 512,
        query_text: str = "",
        **gen_kwargs
    ) -> Tuple[str, Dict]:
        """
        ğŸ”¥ ä½¿ç”¨æ–‡æœ¬æ‹¼æ¥æ¨¡å¼ç”Ÿæˆï¼ˆç¡®ä¿ä½ç½®ç¼–ç æ­£ç¡®ï¼‰
        
        å°†æ‰€æœ‰å†…å®¹æ‹¼æ¥æˆå®Œæ•´æ–‡æœ¬åä¸€æ¬¡æ€§ forwardï¼Œé¿å… KV æ‹¼æ¥å¯¼è‡´çš„ä½ç½®ç¼–ç é”™è¯¯ã€‚
        """
        # 1. æ„å»ºå®Œæ•´çš„ prompt æ–‡æœ¬
        full_prompt_parts = []
        
        # 1.1 å›ºå®šéƒ¨åˆ†ï¼ˆSystem Prompt æˆ– Paper å›ºå®šéƒ¨åˆ†ï¼‰
        if self.mode == "paper":
            fixed_text = self.kv_pool.get_fixed_text('paper_fixed')
        else:
            fixed_text = self.kv_pool.get_fixed_text('system_prompt')
        if fixed_text:
            full_prompt_parts.append(fixed_text)
        
        # 1.2 Shots éƒ¨åˆ†
        if selected_shot_ids:
            if self.mode == "paper":
                # Paper æ¨¡å¼ï¼šquestion-only
                shot_parts = ["You will be provided Problems similar to the ones below:"]
                for sid in selected_shot_ids:
                    if sid in self.kv_pool.kv_cache_pool:
                        example = self.kv_pool.kv_cache_pool[sid]['example']
                        q, _ = self.kv_pool.dataset_handler.format_example_cot(example)
                        shot_parts.append(f"Problem: {q}")
                shot_parts.append("â€”")
                full_prompt_parts.append("\n".join(shot_parts))
            else:
                # CoT/IO æ¨¡å¼ï¼šå®Œæ•´çš„ shot
                fmt = "cot" if self.mode == "cot" else "io"
                shots_text = "\n".join([
                    self.kv_pool.format_shot_text(sid, fmt) 
                    for sid in selected_shot_ids 
                    if sid in self.kv_pool.kv_cache_pool
                ])
                full_prompt_parts.append(shots_text)
        
        # 1.3 Query éƒ¨åˆ†
        header = "When you respond, your last line must be exactly of the form '#### <final_answer>'."
        if self.mode == "cot" or self.mode == "paper":
            formatted_query = f"{header}\nProblem: {query_text}\nSolution:"
        else:
            formatted_query = f"{header}\nProblem: {query_text}\nAnswer:"
        full_prompt_parts.append(formatted_query)
        
        # 2. æ‹¼æ¥å®Œæ•´ prompt
        full_prompt = "\n".join([p for p in full_prompt_parts if p]).strip()
        
        # 3. Tokenize
        inputs = self.tokenizer(full_prompt, return_tensors="pt")
        inputs = {k: v.long() if k == 'input_ids' else v for k, v in inputs.items()}
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        prompt_len = inputs["input_ids"].shape[1]
        print(f"[GEN-TEXT] å®Œæ•´ prompt é•¿åº¦: {prompt_len} tokens")
        
        # 4. ç”Ÿæˆ
        with torch.inference_mode():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=False,
                return_dict_in_generate=True,
                use_cache=True
            )
        
        # 5. è§£ç ç”Ÿæˆçš„éƒ¨åˆ†
        generated_ids = outputs.sequences[0][prompt_len:]
        response = self.tokenizer.decode(generated_ids, skip_special_tokens=True)
        
        print(f"[GEN-TEXT] ç”Ÿæˆå®Œæˆ, è¾“å‡º {len(generated_ids)} tokens")
        
        # 6. ç”Ÿæˆä¿¡æ¯
        gen_info = {
            'num_shots_used': len(selected_shot_ids),
            'total_kv_layers': self.num_layers,
            'total_kv_tokens': prompt_len,
            'output_tokens': len(generated_ids),
            'mode': f"{self.mode}_text_forward",
            'kv_tokens_per_part': {"fixed": 0, "shots": 0, "query": prompt_len}
        }
        
        return response, gen_info
    
    def _generate_with_kv_reuse(
        self,
        selected_shot_ids: List[int],
        prompt_tokens: torch.Tensor,
        query_tokens: torch.Tensor,
        max_new_tokens: int = 512,
        query_text: str = "",
        **gen_kwargs
    ) -> Tuple[str, Dict]:
        """
        ğŸ”¥ KV å¤ç”¨æ¨¡å¼ï¼šå¤ç”¨é¢„è®¡ç®—çš„æ‰€æœ‰ KV cache + RoPE ä½ç½®æ ¡æ­£
        
        è¿™æ˜¯æœ€é«˜æ•ˆçš„æ¨¡å¼ï¼š
        1. System Prompt KV: å¤ç”¨ï¼ˆä½ç½® 0 å¼€å§‹ï¼Œæ— éœ€æ ¡æ­£ï¼‰
        2. Shots KV: å¤ç”¨ + RoPE ä½ç½®æ ¡æ­£ï¼ˆè°ƒæ•´åˆ°æ­£ç¡®çš„ç»å¯¹ä½ç½®ï¼‰
        3. Query: åªè®¡ç®— Query éƒ¨åˆ†çš„ KV
        """
        # ============ æ­¥éª¤ 1: è·å–å›ºå®šéƒ¨åˆ†çš„ KV cache ï¼ˆå·²é¢„è®¡ç®—ï¼Œä½ç½®ä» 0 å¼€å§‹ï¼‰ ============
        if self.mode == "paper":
            K_fixed_layers, V_fixed_layers = self.kv_pool.get_fixed_kv('paper_fixed')
        else:
            K_fixed_layers, V_fixed_layers = self.kv_pool.get_fixed_kv('system_prompt')
        
        fixed_length = K_fixed_layers[0].shape[0] if K_fixed_layers[0].shape[0] > 0 else 0
        current_offset = fixed_length  # ä¸‹ä¸€éƒ¨åˆ†çš„èµ·å§‹ä½ç½®
        
        print(f"[KV-REUSE] å›ºå®šéƒ¨åˆ†: {fixed_length} tokens (ä½ç½® 0-{fixed_length-1}, å¤ç”¨)")
        
        # ============ æ­¥éª¤ 2: è·å–å¹¶æ ¡æ­£é€‰ä¸­ shots çš„ KV ============
        K_shots_all = [[] for _ in range(self.num_layers)]
        V_shots_all = [[] for _ in range(self.num_layers)]
        shots_lengths = []
        
        for shot_id in selected_shot_ids:
            if shot_id not in self.kv_pool.kv_cache_pool:
                continue
            
            # è·å–è¯¥ shot çš„é¢„è®¡ç®— KV
            cache_entry = self.kv_pool.kv_cache_pool[shot_id]
            K_shot = cache_entry['K_layers']
            V_shot = cache_entry['V_layers']
            shot_len = cache_entry['token_count']
            
            # å¯¹æ¯å±‚è¿›è¡Œ RoPE ä½ç½®æ ¡æ­£
            for layer_idx in range(self.num_layers):
                # æ ¡æ­£ K çš„ä½ç½®ï¼ˆä»åŸå§‹ä½ç½® [0, shot_len) è°ƒæ•´åˆ° [current_offset, current_offset+shot_len)ï¼‰
                K_corrected = self.rope_corrector.apply_rope_offset(
                    K_shot[layer_idx], 
                    offset=current_offset
                )
                K_shots_all[layer_idx].append(K_corrected)
                V_shots_all[layer_idx].append(V_shot[layer_idx])  # V ä¸éœ€è¦æ ¡æ­£
            
            shots_lengths.append(shot_len)
            current_offset += shot_len
        
        # æ‹¼æ¥æ‰€æœ‰ shots çš„ KV
        total_shots_tokens = sum(shots_lengths)
        print(f"[KV-REUSE] Shots: {len(selected_shot_ids)} ä¸ª, å…± {total_shots_tokens} tokens (ä½ç½® {fixed_length}-{current_offset-1}, RoPE æ ¡æ­£)")
        
        # ============ æ­¥éª¤ 3: è®¡ç®— Query çš„ KVï¼ˆä½ç½®ä» current_offset å¼€å§‹ï¼‰ ============
        header = "When you respond, your last line must be exactly of the form '#### <final_answer>'."
        if self.mode == "cot" or self.mode == "paper":
            formatted_query = f"{header}\nProblem: {query_text}\nSolution:"
        else:
            formatted_query = f"{header}\nProblem: {query_text}\nAnswer:"
        
        query_inputs = self.tokenizer(formatted_query, return_tensors="pt")
        query_inputs = {k: v.long() if k == 'input_ids' else v for k, v in query_inputs.items()}
        query_inputs = {k: v.to(self.device) for k, v in query_inputs.items()}
        query_length = query_inputs["input_ids"].shape[1]
        
        # è®¾ç½® query çš„ position_idsï¼ˆä» current_offset å¼€å§‹ï¼‰
        query_position_ids = torch.arange(
            current_offset, 
            current_offset + query_length, 
            device=self.device
        ).unsqueeze(0)
        
        print(f"[KV-REUSE] Query: {query_length} tokens (ä½ç½® {current_offset}-{current_offset+query_length-1}, ç°ç®—)")
        
        # ============ æ­¥éª¤ 4: æ‹¼æ¥æ‰€æœ‰ KV ============
        K_all_layers = []
        V_all_layers = []
        
        for layer_idx in range(self.num_layers):
            parts_K = []
            parts_V = []
            
            # 1. å›ºå®šéƒ¨åˆ†
            if K_fixed_layers[layer_idx].shape[0] > 0:
                parts_K.append(K_fixed_layers[layer_idx])
                parts_V.append(V_fixed_layers[layer_idx])
            
            # 2. Shotsï¼ˆå·²æ ¡æ­£ï¼‰
            if K_shots_all[layer_idx]:
                parts_K.extend(K_shots_all[layer_idx])
                parts_V.extend(V_shots_all[layer_idx])
            
            # æ‹¼æ¥
            if parts_K:
                K_concat = torch.cat(parts_K, dim=0)
                V_concat = torch.cat(parts_V, dim=0)
                # ğŸ”¥ ç¡®ä¿ K å’Œ V çš„ dtype ä¸€è‡´ï¼ˆè§£å†³ Half/float ä¸åŒ¹é…é—®é¢˜ï¼‰
                target_dtype = V_concat.dtype  # V æ˜¯åŸå§‹ dtype
                K_concat = K_concat.to(dtype=target_dtype)
            else:
                K_concat = torch.zeros((0, self.kv_pool.d_k), device=self.device)
                V_concat = torch.zeros((0, self.kv_pool.d_v), device=self.device)
            
            K_all_layers.append(K_concat)
            V_all_layers.append(V_concat)
        
        # è½¬æ¢ä¸º past_key_values æ ¼å¼
        past_kv = self._reshape_kv_to_past_format(K_all_layers, V_all_layers)
        past_length = fixed_length + total_shots_tokens
        
        print(f"[KV-REUSE] å¤ç”¨çš„ KV: {past_length} tokens")
        
        # ============ æ­¥éª¤ 5: Forward Queryï¼Œä½¿ç”¨æ‹¼æ¥çš„ KV cache ============
        attention_mask = torch.ones(
            (1, past_length + query_length), 
            dtype=torch.long, 
            device=self.device
        )
        
        with torch.inference_mode():
            outputs = self.model(
                input_ids=query_inputs["input_ids"],
                attention_mask=attention_mask,
                position_ids=query_position_ids,
                past_key_values=past_kv,
                use_cache=True,
                return_dict=True
            )
            
            full_past_kv = outputs.past_key_values
            full_length = past_length + query_length
        
        # ============ æ­¥éª¤ 6: ç”Ÿæˆ ============
        generated_tokens = []
        
        last_token_logits = outputs.logits[0, -1, :]
        next_token = torch.argmax(last_token_logits, dim=-1, keepdim=True)
        generated_tokens.append(next_token.item())
        
        current_token = next_token.unsqueeze(0)
        current_past_kv = full_past_kv
        current_length = full_length
        
        eos_id = getattr(self.tokenizer, "eos_token_id", None)
        decoded_so_far = ""
        
        with torch.inference_mode():
            for step_idx in range(max_new_tokens - 1):
                pos_ids = torch.tensor([[current_length]], device=self.device)
                attn_mask = torch.ones((1, current_length + 1), dtype=torch.long, device=self.device)
                
                outputs = self.model(
                    input_ids=current_token,
                    attention_mask=attn_mask,
                    position_ids=pos_ids,
                    past_key_values=current_past_kv,
                    use_cache=True,
                    return_dict=True
                )
                
                next_token_logits = outputs.logits[0, -1, :]
                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)
                generated_tokens.append(next_token.item())
                
                if eos_id is not None and next_token.item() == eos_id:
                    break
                
                try:
                    decoded_so_far = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
                except Exception:
                    pass
                if "####" in decoded_so_far:
                    break
                
                current_token = next_token.unsqueeze(0)
                current_past_kv = outputs.past_key_values
                current_length += 1
                
                if (step_idx + 1) % 50 == 0:
                    print(f"[KV-REUSE] å·²ç”Ÿæˆ {step_idx + 2} tokens")
        
        response = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
        
        # ç»Ÿè®¡å¤ç”¨ç‡
        reuse_tokens = fixed_length + total_shots_tokens
        total_tokens = reuse_tokens + query_length
        reuse_ratio = reuse_tokens / total_tokens if total_tokens > 0 else 0
        
        print(f"[KV-REUSE] ç”Ÿæˆå®Œæˆ, è¾“å‡º {len(generated_tokens)} tokens")
        print(f"[KV-REUSE] KV å¤ç”¨ç‡: {reuse_ratio:.1%} ({reuse_tokens}/{total_tokens} tokens)")
        
        gen_info = {
            'num_shots_used': len(selected_shot_ids),
            'total_kv_layers': self.num_layers,
            'total_kv_tokens': total_tokens,
            'output_tokens': len(generated_tokens),
            'mode': f"{self.mode}_kv_reuse",
            'kv_tokens_per_part': {
                "fixed": fixed_length,
                "shots": total_shots_tokens,
                "query": query_length,
                "reused": reuse_tokens
            },
            'kv_reuse_ratio': reuse_ratio
        }
        
        return response, gen_info
    
    def _generate_with_hybrid_mode(
        self,
        selected_shot_ids: List[int],
        prompt_tokens: torch.Tensor,
        query_tokens: torch.Tensor,
        max_new_tokens: int = 512,
        query_text: str = "",
        **gen_kwargs
    ) -> Tuple[str, Dict]:
        """
        ğŸ”¥ æ··åˆæ¨¡å¼ç”Ÿæˆï¼šå›ºå®šéƒ¨åˆ†ç”¨é¢„è®¡ç®—çš„ KV cache + åŠ¨æ€éƒ¨åˆ†ç”¨æ–‡æœ¬æ‹¼æ¥
        
        ä¼˜åŒ–ç‚¹ï¼š
        1. System Prompt çš„ KV cache å¯ä»¥å¤ç”¨ï¼Œæ— éœ€æ¯æ¬¡é‡æ–°è®¡ç®—
        2. åŠ¨æ€éƒ¨åˆ†ï¼ˆshots + queryï¼‰çš„ä½ç½®ç¼–ç ä» fixed_length å¼€å§‹ï¼Œä¿è¯è¿ç»­æ€§
        """
        # ============ æ­¥éª¤ 1: è·å–å›ºå®šéƒ¨åˆ†çš„ KV cache ï¼ˆå·²é¢„è®¡ç®—ï¼‰ ============
        if self.mode == "paper":
            K_fixed_layers, V_fixed_layers = self.kv_pool.get_fixed_kv('paper_fixed')
            fixed_text = self.kv_pool.get_fixed_text('paper_fixed')
        else:
            K_fixed_layers, V_fixed_layers = self.kv_pool.get_fixed_kv('system_prompt')
            fixed_text = self.kv_pool.get_fixed_text('system_prompt')
        
        fixed_length = K_fixed_layers[0].shape[0] if K_fixed_layers[0].shape[0] > 0 else 0
        print(f"[HYBRID] å›ºå®šéƒ¨åˆ† KV: {fixed_length} tokens (å·²é¢„è®¡ç®—ï¼Œå¯å¤ç”¨)")
        
        # ============ æ­¥éª¤ 2: æ„å»ºåŠ¨æ€éƒ¨åˆ†çš„æ–‡æœ¬ï¼ˆshots + queryï¼‰ ============
        dynamic_parts = []
        
        # 2.1 Shots éƒ¨åˆ†
        if selected_shot_ids:
            if self.mode == "paper":
                # Paper æ¨¡å¼ï¼šquestion-only
                shot_parts = ["You will be provided Problems similar to the ones below:"]
                for sid in selected_shot_ids:
                    if sid in self.kv_pool.kv_cache_pool:
                        example = self.kv_pool.kv_cache_pool[sid]['example']
                        q, _ = self.kv_pool.dataset_handler.format_example_cot(example)
                        shot_parts.append(f"Problem: {q}")
                shot_parts.append("â€”")
                dynamic_parts.append("\n".join(shot_parts))
            else:
                # CoT/IO æ¨¡å¼ï¼šå®Œæ•´çš„ shot
                fmt = "cot" if self.mode == "cot" else "io"
                shots_text = "\n".join([
                    self.kv_pool.format_shot_text(sid, fmt) 
                    for sid in selected_shot_ids 
                    if sid in self.kv_pool.kv_cache_pool
                ])
                dynamic_parts.append(shots_text)
        
        # 2.2 Query éƒ¨åˆ†
        header = "When you respond, your last line must be exactly of the form '#### <final_answer>'."
        if self.mode == "cot" or self.mode == "paper":
            formatted_query = f"{header}\nProblem: {query_text}\nSolution:"
        else:
            formatted_query = f"{header}\nProblem: {query_text}\nAnswer:"
        dynamic_parts.append(formatted_query)
        
        # æ‹¼æ¥åŠ¨æ€éƒ¨åˆ†
        dynamic_text = "\n".join([p for p in dynamic_parts if p]).strip()
        
        # ============ æ­¥éª¤ 3: Tokenize åŠ¨æ€éƒ¨åˆ† ============
        dynamic_inputs = self.tokenizer(dynamic_text, return_tensors="pt")
        dynamic_inputs = {k: v.long() if k == 'input_ids' else v for k, v in dynamic_inputs.items()}
        dynamic_inputs = {k: v.to(self.device) for k, v in dynamic_inputs.items()}
        
        dynamic_length = dynamic_inputs["input_ids"].shape[1]
        print(f"[HYBRID] åŠ¨æ€éƒ¨åˆ†: {dynamic_length} tokens")
        
        # ============ æ­¥éª¤ 4: å°†å›ºå®šéƒ¨åˆ† KV è½¬æ¢ä¸º past_key_values æ ¼å¼ ============
        if fixed_length > 0:
            fixed_past_kv = self._reshape_kv_to_past_format(K_fixed_layers, V_fixed_layers)
        else:
            fixed_past_kv = None
        
        # ============ æ­¥éª¤ 5: è®¾ç½®æ­£ç¡®çš„ position_idsï¼ˆä» fixed_length å¼€å§‹ï¼‰ ============
        # å…³é”®ï¼è¿™ç¡®ä¿åŠ¨æ€éƒ¨åˆ†çš„ä½ç½®ç¼–ç ä¸å›ºå®šéƒ¨åˆ†è¿ç»­
        position_ids = torch.arange(
            fixed_length, 
            fixed_length + dynamic_length, 
            device=self.device
        ).unsqueeze(0)
        
        # attention_mask éœ€è¦è¦†ç›– past + current
        attention_mask = torch.ones(
            (1, fixed_length + dynamic_length), 
            dtype=torch.long, 
            device=self.device
        )
        
        # ============ æ­¥éª¤ 6: Forward åŠ¨æ€éƒ¨åˆ†ï¼Œä½¿ç”¨å›ºå®šéƒ¨åˆ†çš„ KV cache ============
        with torch.inference_mode():
            # Prefill åŠ¨æ€éƒ¨åˆ†ï¼Œå¤ç”¨å›ºå®šéƒ¨åˆ†çš„ KV
            outputs = self.model(
                input_ids=dynamic_inputs["input_ids"],
                attention_mask=attention_mask,
                position_ids=position_ids,
                past_key_values=fixed_past_kv,
                use_cache=True,
                return_dict=True
            )
            
            # ç°åœ¨ outputs.past_key_values åŒ…å«äº† å›ºå®šéƒ¨åˆ† + åŠ¨æ€éƒ¨åˆ† çš„å®Œæ•´ KV
            full_past_kv = outputs.past_key_values
            full_length = fixed_length + dynamic_length
            
            print(f"[HYBRID] å®Œæ•´ KV: {full_length} tokens (å›ºå®š {fixed_length} + åŠ¨æ€ {dynamic_length})")
        
        # ============ æ­¥éª¤ 7: ä½¿ç”¨å®Œæ•´çš„ KV cache è¿›è¡Œç”Ÿæˆ ============
        generated_tokens = []
        
        # è·å– åŠ¨æ€éƒ¨åˆ†çš„æœ€åä¸€ä¸ª token ä½œä¸ºç”Ÿæˆèµ·ç‚¹
        last_token_logits = outputs.logits[0, -1, :]
        next_token = torch.argmax(last_token_logits, dim=-1, keepdim=True)
        generated_tokens.append(next_token.item())
        
        current_token = next_token.unsqueeze(0)
        current_past_kv = full_past_kv
        current_length = full_length
        
        eos_id = getattr(self.tokenizer, "eos_token_id", None)
        decoded_so_far = ""
        
        with torch.inference_mode():
            for step_idx in range(max_new_tokens - 1):  # -1 å› ä¸ºå·²ç»ç”Ÿæˆäº†ç¬¬ä¸€ä¸ª token
                # position_ids ä¸ºå½“å‰ä½ç½®
                pos_ids = torch.tensor([[current_length]], device=self.device)
                
                # attention_mask è¦†ç›–æ‰€æœ‰å†å² + å½“å‰
                attn_mask = torch.ones((1, current_length + 1), dtype=torch.long, device=self.device)
                
                outputs = self.model(
                    input_ids=current_token,
                    attention_mask=attn_mask,
                    position_ids=pos_ids,
                    past_key_values=current_past_kv,
                    use_cache=True,
                    return_dict=True
                )
                
                # è·å–ä¸‹ä¸€ä¸ª token
                next_token_logits = outputs.logits[0, -1, :]
                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)
                generated_tokens.append(next_token.item())
                
                # æ£€æŸ¥ EOS
                if eos_id is not None and next_token.item() == eos_id:
                    break
                
                # æ£€æŸ¥ #### æ ‡å¿—
                try:
                    decoded_so_far = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
                except Exception:
                    pass
                if "####" in decoded_so_far:
                    break
                
                # æ›´æ–°çŠ¶æ€
                current_token = next_token.unsqueeze(0)
                current_past_kv = outputs.past_key_values
                current_length += 1
                
                # è¿›åº¦æ˜¾ç¤º
                if (step_idx + 1) % 50 == 0:
                    print(f"[HYBRID] å·²ç”Ÿæˆ {step_idx + 2} tokens")
        
        # è§£ç 
        response = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
        print(f"[HYBRID] ç”Ÿæˆå®Œæˆ, è¾“å‡º {len(generated_tokens)} tokens")
        
        # ç”Ÿæˆä¿¡æ¯
        gen_info = {
            'num_shots_used': len(selected_shot_ids),
            'total_kv_layers': self.num_layers,
            'total_kv_tokens': full_length,
            'output_tokens': len(generated_tokens),
            'mode': f"{self.mode}_hybrid",
            'kv_tokens_per_part': {
                "fixed": fixed_length, 
                "dynamic": dynamic_length,
                "total": full_length
            },
            'kv_reuse_ratio': fixed_length / full_length if full_length > 0 else 0
        }
        
        return response, gen_info
    
    def _generate_with_kv_concat(
        self,
        selected_shot_ids: List[int],
        prompt_tokens: torch.Tensor,
        query_tokens: torch.Tensor,
        max_new_tokens: int = 512,
        query_text: str = "",
        **gen_kwargs
    ) -> Tuple[str, Dict]:
        """
        âš ï¸ åŸæœ‰çš„ KV æ‹¼æ¥æ¨¡å¼ï¼ˆä½ç½®ç¼–ç å¯èƒ½æœ‰é—®é¢˜ï¼Œä»…ç”¨äºå®éªŒå¯¹æ¯”ï¼‰
        """
        # æ‹¼è£… KV cache
        past_key_values = self.assemble_kv_for_generation(
            selected_shot_ids, prompt_tokens, query_tokens, query_text
        )
        
        # æ³¨æ„: ç”±äºæˆ‘ä»¬å·²ç»æœ‰äº†å®Œæ•´çš„ KV cacheï¼Œ
        # ç”Ÿæˆæ—¶ä¸éœ€è¦å†è¾“å…¥å®Œæ•´çš„ token åºåˆ—
        # åªéœ€è¦ä¸€ä¸ª dummy input_ids æ¥è§¦å‘ç”Ÿæˆ
        
        use_formatted = False
        formatted_ids = None
        if query_text:
            if self.mode == "cot" or self.mode == "paper":
                formatted_query = f"When you respond, your last line must be exactly of the form '#### <final_answer>'.\nProblem: {query_text}\nSolution:"
            else:
                formatted_query = f"Problem: {query_text}\nAnswer:"
            try:
                formatted_ids = self.tokenizer.encode(formatted_query, add_special_tokens=False, return_tensors='pt')[0]
                use_formatted = True
            except Exception:
                use_formatted = False
        if use_formatted and formatted_ids is not None and formatted_ids.numel() > 0:
            start_token = formatted_ids[-1:].long().unsqueeze(0).to(self.device)
        else:
            start_token = query_tokens[-1:].long().unsqueeze(0).to(self.device)
        
        # è®¡ç®— past_key_values çš„é•¿åº¦ï¼ˆä»ç¬¬ä¸€å±‚çš„ K è·å–ï¼‰
        past_length = past_key_values[0][0].shape[2] if past_key_values else 0
        
        # åˆ›å»º attention_mask
        attention_mask = torch.ones((1, past_length + 1), dtype=torch.long, device=self.device)
        
        # ğŸ”¥ ä½¿ç”¨æ‰‹åŠ¨å¾ªç¯ç”Ÿæˆï¼Œé¿å… cache_position é—®é¢˜
        print(f"[GEN-KV] å¼€å§‹ç”Ÿæˆ (max_tokens={max_new_tokens}, seq_len={past_length})")
        generated_tokens = []
        current_token = start_token
        current_past_kv = past_key_values
        
        eos_id = getattr(self.tokenizer, "eos_token_id", None)
        decoded_so_far = ""
        with torch.inference_mode():
            for step_idx in range(max_new_tokens):
                # Forward
                pos_ids = torch.arange(past_length, past_length + 1, device=self.device).unsqueeze(0)
                outputs = self.model(
                    input_ids=current_token,
                    attention_mask=attention_mask,
                    past_key_values=current_past_kv,
                    position_ids=pos_ids,
                    use_cache=True,
                    return_dict=True
                )
                
                # æ¯ 50 æ­¥æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                if step_idx % 50 == 0 and step_idx > 0:
                    current_total_len = past_length + 1 + step_idx
                    print(f"[GEN-KV] å·²ç”Ÿæˆ {step_idx} tokens, æ€»é•¿åº¦={current_total_len}")
                
                # è·å–ä¸‹ä¸€ä¸ª token
                next_token_logits = outputs.logits[0, -1, :]
                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)
                
                # âœ… ä¿®å¤ï¼šæ·»åŠ  token åˆ°ç”Ÿæˆåˆ—è¡¨ï¼ˆåœ¨æ£€æŸ¥ EOS ä¹‹å‰ï¼‰
                generated_tokens.append(next_token.item())
                
                if eos_id is not None and next_token.item() == eos_id:
                    break
                try:
                    decoded_so_far = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
                except Exception:
                    decoded_so_far = decoded_so_far
                if "####" in decoded_so_far:
                    break
                
                # å‡†å¤‡ä¸‹ä¸€è½®
                current_token = next_token.unsqueeze(0)
                current_past_kv = outputs.past_key_values
                
                # æ›´æ–° attention_mask
                new_seq_len = past_length + 2 + step_idx
                attention_mask = torch.ones((1, new_seq_len), dtype=torch.long, device=self.device)
                past_length = past_length + 1
            
            # ç”Ÿæˆå®Œæˆæç¤º
            print(f"[GEN-KV] ç”Ÿæˆå®Œæˆ, æ€»å…±ç”Ÿæˆ {len(generated_tokens)} tokens")
            # è§£ç 
            response = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
        
        gen_info = {
            'num_shots_used': len(selected_shot_ids),
            'total_kv_layers': self.num_layers,
            'total_kv_tokens': sum(
                self.kv_pool.kv_cache_pool[sid]['token_count'] 
                for sid in selected_shot_ids
            ) + prompt_tokens.shape[0] + query_tokens.shape[0],
            'output_tokens': len(generated_tokens),
            'mode': f"{self.mode}_kv_concat",
            'kv_tokens_per_part': dict(self._last_part_sizes)
        }
        
        return response, gen_info
    
    def _get_paper_question_only_kv(self, shot_ids: List[int]):
        """
        ä¸º Paper æ¨¡å¼è·å– question-only shots çš„ KV
        
        Args:
            shot_ids: Shot ID åˆ—è¡¨
        
        Returns:
            (K_layers, V_layers): æ‹¼è£…å¥½çš„ KV
        """
        # æ„å»º question-onlyéƒ¨åˆ†
        parts = []
        if shot_ids:
            parts.append("You will be provided Problems similar to the ones below:")
            for sid in shot_ids:
                if sid in self.kv_pool.kv_cache_pool:
                    example = self.kv_pool.kv_cache_pool[sid]['example']
                    q, _ = self.kv_pool.dataset_handler.format_example_cot(example)
                    parts.append(f"Problem: {q}")
            parts.append("â€”")  # åˆ†éš”ç¬¦
        
        if parts:
            combined_text = "\n".join(parts)
            K_layers, V_layers = self.kv_pool.get_kv_for_text(combined_text)
        else:
            # è¿”å›ç©º KV
            K_layers = [torch.zeros((0, self.kv_pool.d_k), device=self.device) for _ in range(self.num_layers)]
            V_layers = [torch.zeros((0, self.kv_pool.d_v), device=self.device) for _ in range(self.num_layers)]
        
        return K_layers, V_layers
