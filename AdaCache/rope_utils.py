"""
RoPE ä½ç½®æ ¡æ­£å·¥å…·

ç”¨äºåœ¨æ‹¼æ¥ç‹¬ç«‹è®¡ç®—çš„ KV cache æ—¶ï¼Œæ ¡æ­£ Key ä¸­åµŒå…¥çš„ RoPE ä½ç½®ç¼–ç ã€‚

æ ¸å¿ƒåŸç†:
- RoPE å°†ä½ç½®ä¿¡æ¯åµŒå…¥åˆ° Key ä¸­: K_rotated = K * cos(pos) + rotate_half(K) * sin(pos)
- ç‹¬ç«‹ prefill çš„ shots å„è‡ªä» pos=0 å¼€å§‹
- æ‹¼æ¥æ—¶éœ€è¦å°†æ¯ä¸ª shot çš„ä½ç½®åç§»åˆ°æ­£ç¡®çš„ç»å¯¹ä½ç½®

ä½ç½®æ ¡æ­£å…¬å¼:
  K_corrected = K_old * cos(delta) + rotate_half(K_old) * sin(delta)
  å…¶ä¸­ delta = new_pos - old_pos

æ³¨æ„: V (Value) ä¸éœ€è¦æ ¡æ­£ï¼Œå› ä¸º RoPE åªåº”ç”¨äº Q å’Œ K
"""
import torch
import math
from typing import List, Tuple, Optional


class RoPECorrector:
    """RoPE ä½ç½®æ ¡æ­£å™¨"""
    
    def __init__(self, model, device: str = "npu"):
        """
        åˆå§‹åŒ– RoPE æ ¡æ­£å™¨
        
        Args:
            model: è¯­è¨€æ¨¡å‹ï¼ˆç”¨äºè·å– RoPE å‚æ•°ï¼‰
            device: è®¡ç®—è®¾å¤‡
        """
        self.device = device
        self.model = model
        
        # ä»æ¨¡å‹é…ç½®ä¸­è·å– RoPE å‚æ•°
        config = model.config
        self.head_dim = config.hidden_size // config.num_attention_heads
        self.num_key_value_heads = getattr(config, 'num_key_value_heads', config.num_attention_heads)
        
        # RoPE å‚æ•°
        self.rope_theta = getattr(config, 'rope_theta', 10000.0)
        self.max_position_embeddings = getattr(config, 'max_position_embeddings', 32768)
        
        # é¢„è®¡ç®— cos/sin ç¼“å­˜
        self._build_cos_sin_cache()
        
    def _build_cos_sin_cache(self, max_seq_len: int = None):
        """
        æ„å»º cos/sin ç¼“å­˜ï¼ˆä¸ Qwen2/Llama çš„ RoPE å®ç°ä¸€è‡´ï¼‰
        """
        if max_seq_len is None:
            max_seq_len = self.max_position_embeddings
        
        # è®¡ç®—é¢‘ç‡ï¼ˆä¸ transformers ä¸€è‡´ï¼‰
        dim = self.head_dim
        inv_freq = 1.0 / (self.rope_theta ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim))
        
        # ä½ç½®åºåˆ—
        positions = torch.arange(max_seq_len, dtype=torch.float32)
        
        # å¤–ç§¯è®¡ç®— [seq_len, dim/2]
        freqs = torch.outer(positions, inv_freq)
        
        # æ‰©å±•ä¸º [seq_len, dim] (é‡å¤ä¸¤æ¬¡ä»¥åŒ¹é… head_dim)
        emb = torch.cat([freqs, freqs], dim=-1)
        
        # ç¼“å­˜ cos å’Œ sinï¼ˆğŸ”¥ ä¿å­˜åœ¨ CPU ä¸Šï¼Œé¿å…å¤š NPU è®¾å¤‡åˆ‡ç‰‡é—®é¢˜ï¼‰
        self.cos_cache = emb.cos()  # [max_seq_len, head_dim]
        self.sin_cache = emb.sin()  # [max_seq_len, head_dim]
        
    def _rotate_half(self, x: torch.Tensor) -> torch.Tensor:
        """
        æ—‹è½¬å‘é‡çš„ä¸€åŠï¼ˆRoPE æ ‡å‡†æ“ä½œï¼‰
        
        Args:
            x: [..., head_dim]
        
        Returns:
            rotated: [..., head_dim]
        """
        x1 = x[..., :x.shape[-1] // 2]
        x2 = x[..., x.shape[-1] // 2:]
        return torch.cat([-x2, x1], dim=-1)
    
    def apply_rope_offset(
        self,
        K: torch.Tensor,
        offset: int
    ) -> torch.Tensor:
        """
        å¯¹ Key åº”ç”¨ RoPE ä½ç½®åç§»
        
        åŸç†: å‡è®¾ K æ˜¯ç”¨ pos=[0,1,...,T-1] è®¡ç®—çš„
              æˆ‘ä»¬éœ€è¦å°†å…¶è½¬æ¢ä¸º pos=[offset, offset+1, ..., offset+T-1]
        
        Args:
            K: Key å¼ é‡ [seq_len, d_k] å…¶ä¸­ d_k = num_heads * head_dim
            offset: ä½ç½®åç§»é‡
        
        Returns:
            K_corrected: æ ¡æ­£åçš„ Key [seq_len, d_k]ï¼Œä¿æŒåŸå§‹ dtype
        """
        if offset == 0:
            return K  # æ— éœ€æ ¡æ­£
        
        seq_len, d_k = K.shape
        original_dtype = K.dtype  # ğŸ”¥ ä¿å­˜åŸå§‹æ•°æ®ç±»å‹
        original_device = K.device  # ğŸ”¥ ä¿å­˜åŸå§‹è®¾å¤‡
        
        # ç¡®ä¿ offset ä¸è¶…è¿‡ç¼“å­˜èŒƒå›´
        if offset + seq_len > self.cos_cache.shape[0]:
            self._build_cos_sin_cache(offset + seq_len + 1024)
        
        # Reshape: [seq_len, d_k] -> [seq_len, num_heads, head_dim]
        K_reshaped = K.view(seq_len, self.num_key_value_heads, self.head_dim)
        
        # è·å–æ—§ä½ç½®å’Œæ–°ä½ç½®çš„ cos/sin
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå…ˆåˆ‡ç‰‡ï¼Œå†è½¬æ¢è®¾å¤‡å’Œ dtypeï¼ˆé¿å…è®¾å¤‡ä¸åŒ¹é…ï¼‰
        old_positions = torch.arange(seq_len, device='cpu')  # åœ¨ CPU ä¸Šåˆ›å»ºç´¢å¼•
        new_positions = old_positions + offset
        
        # ğŸ”¥ å…ˆåœ¨ CPU ä¸Šåˆ‡ç‰‡ï¼Œç„¶åè½¬æ¢åˆ°ç›®æ ‡è®¾å¤‡å’Œ dtype
        cos_old = self.cos_cache[old_positions].to(device=original_device, dtype=original_dtype)  # [seq_len, head_dim]
        sin_old = self.sin_cache[old_positions].to(device=original_device, dtype=original_dtype)
        cos_new = self.cos_cache[new_positions].to(device=original_device, dtype=original_dtype)
        sin_new = self.sin_cache[new_positions].to(device=original_device, dtype=original_dtype)
        
        # æ‰©å±•ç»´åº¦ä»¥åŒ¹é… num_heads
        cos_old = cos_old.unsqueeze(1)  # [seq_len, 1, head_dim]
        sin_old = sin_old.unsqueeze(1)
        cos_new = cos_new.unsqueeze(1)
        sin_new = sin_new.unsqueeze(1)
        
        # åæ—‹è½¬ï¼ˆå»é™¤æ—§ä½ç½®çš„ RoPEï¼‰
        # RoPE_inv(a): x * cos(a) - rotate_half(x) * sin(a)
        K_unrotated = K_reshaped * cos_old - self._rotate_half(K_reshaped) * sin_old
        
        # æ­£æ—‹è½¬ï¼ˆåº”ç”¨æ–°ä½ç½®çš„ RoPEï¼‰
        # RoPE(b): x * cos(b) + rotate_half(x) * sin(b)
        K_corrected = K_unrotated * cos_new + self._rotate_half(K_unrotated) * sin_new
        
        # Reshape å› [seq_len, d_k]
        K_corrected = K_corrected.view(seq_len, d_k)
        
        # ğŸ”¥ ç¡®ä¿è¿”å›ä¸è¾“å…¥ç›¸åŒçš„ dtypeï¼ˆå…³é”®ï¼é˜²æ­¢ float32 å’Œ float16 ä¸åŒ¹é…ï¼‰
        return K_corrected.to(dtype=original_dtype)
    
    def correct_kv_positions(
        self,
        K_layers: List[torch.Tensor],
        V_layers: List[torch.Tensor],
        offset: int
    ) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:
        """
        å¯¹æ‰€æœ‰å±‚çš„ KV è¿›è¡Œä½ç½®æ ¡æ­£
        
        Args:
            K_layers: æ‰€æœ‰å±‚çš„ Key [layer][seq_len, d_k]
            V_layers: æ‰€æœ‰å±‚çš„ Value [layer][seq_len, d_v]
            offset: ä½ç½®åç§»é‡
        
        Returns:
            (K_corrected, V_layers): æ ¡æ­£åçš„ K å’ŒåŸå§‹ Vï¼ˆV ä¸éœ€è¦æ ¡æ­£ï¼‰
        """
        if offset == 0:
            return K_layers, V_layers
        
        K_corrected = []
        for layer_idx, K in enumerate(K_layers):
            K_new = self.apply_rope_offset(K, offset)
            K_corrected.append(K_new)
        
        # V ä¸éœ€è¦æ ¡æ­£ï¼ˆRoPE åªåº”ç”¨äº Q å’Œ Kï¼‰
        return K_corrected, V_layers


def test_rope_corrector():
    """æµ‹è¯• RoPE æ ¡æ­£å™¨"""
    print("Testing RoPE Corrector...")
    
    # åˆ›å»ºå‡çš„æ¨¡å‹é…ç½®
    class FakeConfig:
        hidden_size = 2048
        num_attention_heads = 16
        num_key_value_heads = 4
        rope_theta = 10000.0
        max_position_embeddings = 4096
    
    class FakeModel:
        config = FakeConfig()
    
    corrector = RoPECorrector(FakeModel(), device="cpu")
    
    # æµ‹è¯•æ•°æ®
    seq_len = 10
    d_k = FakeConfig.num_key_value_heads * (FakeConfig.hidden_size // FakeConfig.num_attention_heads)
    K = torch.randn(seq_len, d_k)
    
    # æµ‹è¯•åç§»
    K_offset_0 = corrector.apply_rope_offset(K, offset=0)
    assert torch.allclose(K, K_offset_0), "offset=0 should return identical K"
    
    K_offset_10 = corrector.apply_rope_offset(K, offset=10)
    assert K_offset_10.shape == K.shape, "Shape should be preserved"
    
    print("âœ“ RoPE Corrector test passed!")


if __name__ == "__main__":
    test_rope_corrector()
