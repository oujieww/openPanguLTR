"""
Many-Shot KV Cache ä¸»è¿è¡Œè„šæœ¬
åŸºäº KV cache çš„å®Œæ•´æ£€ç´¢ä¸å¤ç”¨ç³»ç»Ÿ (åä¸º 910B NPU)

å®Œæ•´æµç¨‹:
1. ç¦»çº¿ Prefilling: æ„å»º 1024-shot KV cache æ± 
2. Query ç¼–ç : æå–å¹³å‡ Q å‘é‡
3. Shot æ’åº: Tokençº§æ‰“åˆ† -> Shotçº§èšåˆ
4. æ¢é’ˆé€‰æ‹©: æŒ‰çª—å£né€è½®æ‰©å±•ï¼Œç†µåˆ¤æ–­åœæ­¢
5. KV æ‹¼è£…: shots KV + prompt+query KV
6. æœ€ç»ˆç”Ÿæˆ: ä½¿ç”¨æ‹¼è£…çš„ KV cache
"""
import sys
import os
import argparse
import logging
import torch
from pathlib import Path
from datetime import datetime

os.environ.setdefault("HF_ENDPOINT", "https://hf-mirror.com")
os.environ.setdefault("HF_HUB_ENABLE_HF_TRANSFER", "1")
os.environ.setdefault("HF_HUB_HTTP_TIMEOUT", "60")
# Merge: ä¿®æ”¹ HF_HOME è·¯å¾„ä¸ºå…±äº«å­˜å‚¨è·¯å¾„
Original: os.environ.setdefault("HF_HOME", "/data/oujie/models/hf_home")
# os.environ.setdefault("HF_HOME", "/home/models/oujie-data/hf_home")
# Merge: ä¿®æ”¹ HF_DATASETS_CACHE è·¯å¾„ä¸ºå…±äº«å­˜å‚¨è·¯å¾„
Original: os.environ.setdefault("HF_DATASETS_CACHE", "/data/oujie/models/hf_home/datasets")
# os.environ.setdefault("HF_DATASETS_CACHE", "/home/models/oujie-data/hf_home/datasets")
# Merge: æ·»åŠ  HF_TOKEN è®¾ç½®
# Original: # os.environ.setdefault("HF_TOKEN", "YOUR_HF_TOKEN") # è¯·é€šè¿‡ç¯å¢ƒå˜é‡ HF_TOKEN è®¾ç½®
# ä»ç¯å¢ƒå˜é‡è·å– HF_TOKENï¼Œå¦‚æœæœªè®¾ç½®åˆ™ä¸è®¾ç½®
# os.environ.setdefault("HF_TOKEN", "YOUR_HF_TOKEN")  # è¯·é€šè¿‡ç¯å¢ƒå˜é‡ HF_TOKEN è®¾ç½®

# æ·»åŠ æ ¹è·¯å¾„ä»¥å®šä½ util åŒ…
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from transformers import AutoTokenizer, AutoModelForCausalLM
from util.dataset_handlers import get_dataset_handler

from config import AdaCacheConfig, create_config_from_args
from manyshot_kv_core import ManyShotKVEvaluator
from model_utils import need_trust_remote_code
from generate_manyshot_summary import generate_summary_from_manyshot_results


def safe_model_id(model_name: str) -> str:
    """å°†æ¨¡å‹åç§°è½¬æ¢ä¸ºå®‰å…¨çš„æ–‡ä»¶å"""
    return model_name.rstrip("/").split("/")[-1]


def _detect_device() -> str:
    """æ£€æµ‹å¯ç”¨è®¾å¤‡"""
    if os.environ.get("FORCE_NPU", "").lower() in ("1", "true", "yes"):
        return "npu"
    if hasattr(torch, "npu"):
        try:
            if torch.npu.is_available():
                return "npu"
        except Exception:
            pass
    if torch.cuda.is_available():
        return "cuda"
    return "cpu"


def load_model(model_name: str, model_root: str, device_kind: str, use_sharding: bool = True):
    """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
    if os.path.isabs(model_name):
        local_path = model_name
    else:
        local_path = os.path.join(model_root, model_name)
    
    if not os.path.exists(local_path):
        raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {local_path}")
    
    logging.info(f"åŠ è½½æ¨¡å‹: {local_path}")
    
    need_trust_remote = need_trust_remote_code(model_name)
    if "pangu" in model_name.lower():
        need_trust_remote = True
    
    logging.info(f"æ¨¡å‹ç±»å‹æ£€æµ‹: {'éœ€è¦' if need_trust_remote else 'ä¸éœ€è¦'} trust_remote_code")
    # Merge: ç§»é™¤ local_files_only=True ä»¥æ”¯æŒåœ¨çº¿ä¸‹è½½
    # Original: tokenizer = AutoTokenizer.from_pretrained(
    #     local_path,
    #     trust_remote_code=need_trust_remote,
    #     use_fast=True,
    #     local_files_only=True
    # )
    tokenizer = AutoTokenizer.from_pretrained(
        local_path, 
        trust_remote_code=need_trust_remote, 
        use_fast=True
    )
    
    dtype = torch.float16 if device_kind in ("cuda", "npu") else None
    
    npu_cnt = 0
    if device_kind == "npu":
        try:
            npu_cnt = torch.npu.device_count()
        except Exception:
            npu_cnt = 1
    
    use_sharding = (device_kind == "npu" and npu_cnt > 1 and use_sharding)
    
    if use_sharding:
        logging.info(f"ä½¿ç”¨åˆ†ç‰‡æ¨¡å¼åŠ è½½æ¨¡å‹ (device_map=auto)")
        # Merge: ç§»é™¤ local_files_only=True ä»¥æ”¯æŒåœ¨çº¿ä¸‹è½½
        # Original: model = AutoModelForCausalLM.from_pretrained(
        #     local_path,
        #     trust_remote_code=need_trust_remote,
        #     torch_dtype=dtype,
        #     low_cpu_mem_usage=True,
        #     device_map="auto",
        #     local_files_only=True
        # )
        model = AutoModelForCausalLM.from_pretrained(
            local_path,
            trust_remote_code=need_trust_remote,
            torch_dtype=dtype,
            low_cpu_mem_usage=True,
            device_map="auto"
        )
        logging.info(f"æ¨¡å‹åˆ†ç‰‡æƒ…å†µ: {getattr(model, 'hf_device_map', 'N/A')}")
    else:
        target = "npu" if device_kind == "npu" else ("cuda" if device_kind == "cuda" else "cpu")
        logging.info(f"åŠ è½½æ¨¡å‹åˆ° {target}")
        # Merge: ç§»é™¤ local_files_only=True ä»¥æ”¯æŒåœ¨çº¿ä¸‹è½½
        # Original: model = AutoModelForCausalLM.from_pretrained(
        #     local_path,
        #     trust_remote_code=need_trust_remote,
        #     torch_dtype=dtype,
        #     low_cpu_mem_usage=True,
        #     local_files_only=True
        # )
        model = AutoModelForCausalLM.from_pretrained(
            local_path,
            trust_remote_code=need_trust_remote,
            torch_dtype=dtype,
            low_cpu_mem_usage=True
        )
        model.to(target)
    
    model.eval()
    
    if device_kind == "cuda":
        logging.info(f"ä½¿ç”¨ CUDA (GPU æ•°é‡: {torch.cuda.device_count()})")
    elif device_kind == "npu":
        logging.info(f"ä½¿ç”¨ NPU (NPU æ•°é‡: {npu_cnt})")
    else:
        logging.info("ä½¿ç”¨ CPU")
    
    return tokenizer, model


# Merge: æ·»åŠ  tasks å‚æ•°æ”¯æŒ
# Original: def run_manyshot_kv_experiment(
#     config: AdaCacheConfig,
#     model_name: str,
#     dataset_name: str,
#     dataset_subset: str = None
# ):
def run_manyshot_kv_experiment(
    config: AdaCacheConfig,
    model_name: str,
    dataset_name: str,
    dataset_subset: str = None,
    tasks: str = None  # ğŸ”¥ æ·»åŠ  tasks å‚æ•°
):
    """
    è¿è¡Œ Many-Shot KV Cache å®éªŒ
    
    Args:
        config: é…ç½®å¯¹è±¡
        model_name: æ¨¡å‹åç§°
        dataset_name: æ•°æ®é›†åç§°
        dataset_subset: æ•°æ®é›†å­é›†
        tasks: ä»»åŠ¡åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰ï¼Œç”¨äºè¿‡æ»¤ CoT-Collection ç­‰æ•°æ®é›†
    """
    device_kind = config.device or _detect_device()
    
    tokenizer, model = load_model(model_name, config.model_root, device_kind, config.use_sharding)
    
    # Merge: æ·»åŠ  tasks å‚æ•°ä¼ é€’
    # Original: dataset_handler = get_dataset_handler(dataset_name, dataset_subset)
    dataset_handler = get_dataset_handler(dataset_name, dataset_subset, tasks)
    
    logging.info(f"åŠ è½½æµ‹è¯•é›†: {dataset_name}/{dataset_subset or 'default'}")
    _, test_set = dataset_handler.load_and_split(test_size=config.eval_samples, seed=config.seed)
    logging.info(f"æµ‹è¯•é›†å¤§å°: {len(test_set)}")
    
    evaluator = ManyShotKVEvaluator(
        model=model,
        tokenizer=tokenizer,
        config=config,
        dataset_name=dataset_name,
        dataset_subset=dataset_subset,
        device=device_kind
    )
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_id = safe_model_id(model_name)
    # Merge: æ·»åŠ å†’å·æ›¿æ¢ä»¥æ”¯æŒæ›´å¤šæ ¼å¼
    # Original: dataset_id = dataset_name.replace('/', '_')
    dataset_id = dataset_name.replace('/', '_').replace(':', '_')
    if dataset_subset:
        dataset_id += f"_{dataset_subset}"
    
    run_mode = f"manyshot_kv_{dataset_id}"
    run_root = os.path.join(config.output_dir, run_mode, config.run_id)
    os.makedirs(run_root, exist_ok=True)
    
    try:
        mode_root = os.path.join(config.output_dir, run_mode)
        latest_link = os.path.join(mode_root, "latest")
        if os.path.islink(latest_link):
            os.unlink(latest_link)
        elif os.path.exists(latest_link):
            pass
        rel_target = os.path.relpath(run_root, mode_root)
        os.symlink(rel_target, latest_link)
    except Exception:
        pass
    
    file_prefix = f"{model_id}_w{config.window_size}_tau{config.entropy_threshold}_{timestamp}"
    output_prefix = os.path.join(run_root, file_prefix)
    
    logging.info("=" * 80)
    logging.info(f"å¼€å§‹ Many-Shot KV Cache è¯„ä¼°")
    logging.info(f"  æ¨¡å‹: {model_name}")
    logging.info(f"  æ•°æ®é›†: {dataset_name}/{dataset_subset or 'default'}")
    logging.info(f"  è®¾å¤‡: {device_kind}")
    logging.info(f"  å…¨å±€æ± å¤§å°: {config.global_pool_size}")
    logging.info(f"  çª—å£å¤§å°: {config.window_size}")
    logging.info(f"  ç†µé˜ˆå€¼: {config.entropy_threshold}")
    logging.info("=" * 80)
    
    metrics = evaluator.evaluate(test_set, output_prefix)
    
    del model
    del tokenizer
    if device_kind == "cuda":
        torch.cuda.empty_cache()
    elif device_kind == "npu":
        try:
            torch.npu.empty_cache()
        except Exception:
            pass
    
    return metrics


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Many-Shot KV Cache - åŸºäºKVç¼“å­˜çš„æ£€ç´¢ä¸å¤ç”¨ç³»ç»Ÿ")
    
    # åŸºç¡€å‚æ•°
    parser.add_argument("--mode", type=str, default="cot", choices=["cot", "io", "paper"], help="æ¨¡å¼ï¼šcot/io/paper")
    parser.add_argument("--global_pool_size", type=int, default=1024, help="å…¨å±€ç¤ºä¾‹æ± å¤§å°")
    parser.add_argument("--window_size", type=int, default=4, help="æ¢é’ˆçª—å£å¤§å°")
    parser.add_argument("--entropy_threshold", type=float, default=0.5, help="ç†µé˜ˆå€¼")
    parser.add_argument("--max_probe_rounds", type=int, default=256, help="æœ€å¤§æ¢é’ˆè½®æ•°")
    
    # æ¨¡å‹ä¸æ•°æ®é›†
    parser.add_argument("--models", type=str, 
                       default="/data/oujie/models/pangu/FreedomIntelligence/openPangu-Embedded-1B-V1.1",
                       help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--datasets", type=str, default="openai/gsm8k:main",
                       help="æ•°æ®é›†é…ç½®")
    # Merge: æ·»åŠ  tasks å‚æ•°
    # Original: æ— æ­¤å‚æ•°
    parser.add_argument("--tasks", type=str, default=None,
                       help="ä»»åŠ¡åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰ï¼Œç”¨äºè¿‡æ»¤ CoT-Collection ç­‰æ•°æ®é›†")
    
    # å®éªŒé…ç½®
    parser.add_argument("--eval_samples", type=int, default=100, help="è¯„æµ‹æ ·æœ¬æ•°")
    parser.add_argument("--gen_tokens", type=int, default=512, help="ç”Ÿæˆ token æ•°é‡")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    
    # è·¯å¾„é…ç½®
    parser.add_argument("--model_root", type=str, default="/home/models/models/", help="æ¨¡å‹æ ¹ç›®å½•")
    parser.add_argument("--output_dir", type=str, default="./outputs", help="è¾“å‡ºç›®å½•")
    
    # è¿è¡Œé…ç½®
    parser.add_argument("--run_id", type=str, default="manyshot_kv_test", help="è¿è¡Œ ID")
    parser.add_argument("--verbose", action="store_true", help="è¯¦ç»†æ—¥å¿—")
    
    args = parser.parse_args()
    
    log_level = logging.INFO
    os.makedirs(os.path.join(args.output_dir, "logs"), exist_ok=True)
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler(os.path.join(args.output_dir, "logs", f"manyshot_kv_{args.run_id}.log"))
        ]
    )
    
    config = create_config_from_args(args)
    
    models = [m.strip() for m in args.models.split(',') if m.strip()]
    
    datasets = []
    for ds in args.datasets.split(','):
        ds = ds.strip()
        if ':' in ds:
            name, subset = ds.split(':', 1)
            datasets.append({"name": name.strip(), "subset": subset.strip()})
        else:
            datasets.append({"name": ds, "subset": None})
    
    logging.info("=" * 80)
    logging.info("Many-Shot KV Cache å®éªŒé…ç½®")
    logging.info("=" * 80)
    logging.info(f"æ¨¡å‹æ•°é‡: {len(models)}")
    logging.info(f"æ•°æ®é›†æ•°é‡: {len(datasets)}")
    logging.info(f"å…¨å±€æ± å¤§å°: {config.global_pool_size}")
    logging.info(f"çª—å£å¤§å°: {config.window_size}")
    logging.info(f"ç†µé˜ˆå€¼: {config.entropy_threshold}")
    logging.info(f"è¯„æµ‹æ ·æœ¬æ•°: {config.eval_samples}")
    logging.info("=" * 80)
    
    all_results = []
    
    for model_name in models:
        for dataset_config in datasets:
            dataset_name = dataset_config["name"]
            dataset_subset = dataset_config["subset"]
            
            try:
                logging.info("\n\n")
                logging.info("#" * 80)
                logging.info(f"# å®éªŒ: {model_name} Ã— {dataset_name}/{dataset_subset or 'default'}")
                logging.info("#" * 80)
                
                # Merge: ä¼ é€’ tasks å‚æ•°
                # Original: metrics = run_manyshot_kv_experiment(
                #     config=config,
                #     model_name=model_name,
                #     dataset_name=dataset_name,
                #     dataset_subset=dataset_subset
                # )
                metrics = run_manyshot_kv_experiment(
                    config=config,
                    model_name=model_name,
                    dataset_name=dataset_name,
                    dataset_subset=dataset_subset,
                    tasks=args.tasks  # ğŸ”¥ ä¼ é€’ tasks å‚æ•°
                )
                
                all_results.append({
                    "model": model_name,
                    "dataset": f"{dataset_name}/{dataset_subset or 'default'}",
                    "metrics": metrics
                })
                
            except Exception as e:
                logging.error(f"å®éªŒå¤±è´¥: {e}", exc_info=True)
    
    logging.info("\n\n")
    logging.info("=" * 80)
    logging.info("æ‰€æœ‰å®éªŒæ±‡æ€»")
    logging.info("=" * 80)
    for result in all_results:
        logging.info(f"\næ¨¡å‹: {result['model']}")
        logging.info(f"æ•°æ®é›†: {result['dataset']}")
        logging.info(f"  å‡†ç¡®ç‡: {result['metrics']['acc_numeric']:.4f}")
        logging.info(f"  å¹³å‡ shot æ•°: {result['metrics']['num_shots_mean']:.2f}")
    
    logging.info("\nâœ“ æ‰€æœ‰å®éªŒå®Œæˆï¼")

if __name__ == "__main__":
    main()
