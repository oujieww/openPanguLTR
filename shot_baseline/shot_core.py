"""
Shot Baseline æ ¸å¿ƒè¯„ä¼°æ¨¡å—
æ”¯æŒä¸‰ç§æ¨¡å¼ï¼šCoT (Chain-of-Thought), IO (Input-Output), Paper
"""
import os
import sys
import json
import time
import logging
import random
import numpy as np
import torch
from typing import Dict, List, Tuple
from tqdm import tqdm

# æ·»åŠ æ ¹è·¯å¾„ä»¥å®šä½ util åŒ…
base_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if base_path not in sys.path:
    sys.path.insert(0, base_path)

# ä» util å¯¼å…¥è¯„ä¼°å‡½æ•°
try:
    from util.new_metrics import evaluate_answer
except ImportError:
    from util.metrics_utils import (
        normalize_answer,
        token_f1_pair,
        parse_number_from_text,
        numeric_equal,
        relative_error
    )
    
    def evaluate_answer(pred_final: str, gold_final: str) -> dict:
        """è¯„ä¼°ç­”æ¡ˆï¼ˆå¤‡ç”¨ç‰ˆæœ¬ï¼‰"""
        em_str = 1.0 if normalize_answer(pred_final) == normalize_answer(gold_final) else 0.0
        contains = 1.0 if normalize_answer(gold_final) in normalize_answer(pred_final) else 0.0
        tf1 = token_f1_pair(pred_final, gold_final)
        
        gold_num = parse_number_from_text(gold_final)
        pred_num = parse_number_from_text(pred_final)
        
        numeric_ok = 0
        relerr = None
        
        if gold_num is not None and pred_num is not None:
            numeric_ok = 1 if numeric_equal(pred_num, gold_num) else 0
            relerr = relative_error(pred_num, gold_num)
        
        return {
            "numeric_ok": numeric_ok,
            "relerr": relerr,
            "em_str": em_str,
            "contains": contains,
            "tf1": tf1
        }


class ShotEvaluator:
    """Shot Baseline è¯„ä¼°å™¨"""
    
    def __init__(
        self,
        model,
        tokenizer,
        config,
        dataset_handler,
        dataset_name: str,
        dataset_subset: str = None,
        train_pool: List = None
    ):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            model: è¯­è¨€æ¨¡å‹
            tokenizer: åˆ†è¯å™¨
            config: Shot é…ç½®
            dataset_handler: æ•°æ®é›†å¤„ç†å™¨
            dataset_name: æ•°æ®é›†åç§°
            dataset_subset: æ•°æ®é›†å­é›†
            train_pool: è®­ç»ƒé›†ç¤ºä¾‹æ± ï¼ˆç”¨äºéšæœºé€‰æ‹© shotsï¼‰
        """
        self.model = model
        self.tokenizer = tokenizer
        self.config = config
        self.dataset_handler = dataset_handler
        self.dataset_name = dataset_name
        self.dataset_subset = dataset_subset
        self.train_pool = train_pool or []
        
        logging.info(f"ShotEvaluator åˆå§‹åŒ–å®Œæˆ: mode={config.mode}, dataset={dataset_name}/{dataset_subset}")
        logging.info(f"è®­ç»ƒæ± å¤§å°: {len(self.train_pool)}")
    
    def _get_system_prompt(self) -> str:
        """æ ¹æ®æ•°æ®é›†ç±»å‹è·å– system prompt"""
        name_l = (self.dataset_name or "").lower()
        
        # CoT-Collection æ•°æ®é›†ï¼ˆåŒ»ç–—ã€NLI ç­‰å¤šç§ä»»åŠ¡ï¼‰
        if "cot-collection" in name_l or "modelscope" in name_l:
            return (
                "You are a helpful assistant. "
                "Carefully analyze the given task and provide your reasoning step by step. "
                "Then provide the final answer clearly at the end."
            )
        
        if "aqua" in name_l:
            return (
                "You are a precise math assistant. "
                "Answer multiple choice questions by analyzing the options and providing the correct letter (A, B, C, D, or E). "
                "End your response with 'The answer is X' where X is the correct option letter."
            )
        
        if "physics" in name_l:
            return (
                "You are a physics expert. "
                "Solve the given physics problems step by step. "
                "Provide the final numerical answer clearly at the end."
            )
        
        if "competition_math" in name_l or "math" in name_l:
            return (
                "You are an expert mathematician. "
                "Solve the given mathematical problems step by step. "
                "Show your work clearly and provide the final answer. "
                "If applicable, use boxed{} to highlight the final answer."
            )
        
        if "svamp" in name_l:
            return (
                "You are a helpful math assistant. "
                "Solve the given word problem step by step. "
                "Provide only the numerical answer at the end."
            )
        
        # é»˜è®¤ GSM8K é£æ ¼ - å¼ºè°ƒæ­£ç¡®çš„ç­”æ¡ˆæ ¼å¼
        return (
            "You are a precise math assistant. "
            "Solve the given math problem step by step. "
            "IMPORTANT: End your answer with '#### <number>' where <number> is the final numerical answer. "
            "The format must be '####' followed by a space and then the number. "
            "For example: '#### 42' or '#### 3.14'. "
            "Do NOT write the number before ####."
        )
    
    def _select_random_shots(self, test_question: str, num_shots: int) -> List:
        """
        éšæœºé€‰æ‹© shotsï¼Œç¡®ä¿ä¸æµ‹è¯•é—®é¢˜ä¸é‡å¤
        
        Args:
            test_question: æµ‹è¯•é—®é¢˜
            num_shots: éœ€è¦çš„ shot æ•°é‡
        
        Returns:
            é€‰ä¸­çš„ç¤ºä¾‹åˆ—è¡¨
        """
        if not self.train_pool or num_shots <= 0:
            return []
        
        # è¿‡æ»¤æ‰ä¸æµ‹è¯•é—®é¢˜ç›¸åŒçš„ç¤ºä¾‹
        test_q_norm = test_question.strip().lower()
        available_pool = []
        for ex in self.train_pool:
            q, _ = self.dataset_handler.format_example_cot(ex)
            if q.strip().lower() != test_q_norm:
                available_pool.append(ex)
        
        if not available_pool:
            logging.warning(f"æ²¡æœ‰å¯ç”¨çš„ç¤ºä¾‹ï¼ˆè®­ç»ƒæ± å¤§å°: {len(self.train_pool)}ï¼‰")
            return []
        
        # éšæœºé€‰æ‹©
        actual_num = min(num_shots, len(available_pool))
        selected = random.sample(available_pool, actual_num)
        
        return selected
    
    def _build_prompt_cot(self, test_question: str, num_shots: int) -> Tuple[str, int]:
        """
        æ„å»º CoT æ¨¡å¼çš„ prompt
        æ¯ä¸ª shot åŒ…å«ï¼šé—®é¢˜ + è§£ç­”è¿‡ç¨‹ + ç­”æ¡ˆ
        
        Returns:
            (prompt_text, actual_num_shots)
        """
        shots = self._select_random_shots(test_question, num_shots)
        
        examples_text = []
        for ex in shots:
            q, cot = self.dataset_handler.format_example_cot(ex)
            examples_text.append(f"Problem: {q}\nSolution: {cot}")
        
        if examples_text:
            prompt = "\n\n".join(examples_text) + f"\n\nProblem: {test_question}\nSolution:"
        else:
            prompt = f"Problem: {test_question}\nSolution:"
        
        return prompt, len(shots)
    
    def _build_prompt_io(self, test_question: str, num_shots: int) -> Tuple[str, int]:
        """
        æ„å»º IO æ¨¡å¼çš„ prompt
        æ¯ä¸ª shot åŒ…å«ï¼šé—®é¢˜ + ç­”æ¡ˆï¼ˆæ— è§£ç­”è¿‡ç¨‹ï¼‰
        
        Returns:
            (prompt_text, actual_num_shots)
        """
        shots = self._select_random_shots(test_question, num_shots)
        
        examples_text = []
        for ex in shots:
            q, _ = self.dataset_handler.format_example_cot(ex)
            answer = self.dataset_handler.extract_gold_answer(ex)
            examples_text.append(f"Problem: {q}\nAnswer: {answer}")
        
        if examples_text:
            prompt = "\n\n".join(examples_text) + f"\n\nProblem: {test_question}\nAnswer:"
        else:
            prompt = f"Problem: {test_question}\nAnswer:"
        
        return prompt, len(shots)
    
    def _build_prompt_paper(self, test_question: str, k_full: int, num_questions: int) -> Tuple[str, int, int]:
        """
        æ„å»º Paper æ¨¡å¼çš„ prompt
        æŒ‰ç…§ AdaCache æ ¼å¼ï¼š
        1. å…ˆå±•ç¤º question-only shotsï¼ˆç±»ä¼¼ç¤ºä¾‹ï¼‰
        2. æ·»åŠ åˆ†éš”ç¬¦å’Œæç¤º
        3. å†å±•ç¤º fullshotsï¼ˆå®Œæ•´ç¤ºä¾‹ï¼‰
        4. æœ€åæ˜¯æµ‹è¯•é—®é¢˜
        
        Returns:
            (prompt_text, actual_k_full, actual_num_questions)
        """
        # é€‰æ‹© fullshotsï¼ˆé—®é¢˜ + è¿‡ç¨‹ + ç­”æ¡ˆï¼‰
        fullshots = self._select_random_shots(test_question, k_full)
        
        # é€‰æ‹© question-only shotsï¼ˆåªæœ‰é—®é¢˜ï¼‰
        # éœ€è¦ç¡®ä¿ä¸ä¸ fullshots å’Œæµ‹è¯•é—®é¢˜é‡å¤
        used_questions = set()
        for ex in fullshots:
            q, _ = self.dataset_handler.format_example_cot(ex)
            used_questions.add(q.strip().lower())
        used_questions.add(test_question.strip().lower())
        
        question_only_pool = []
        for ex in self.train_pool:
            q, _ = self.dataset_handler.format_example_cot(ex)
            if q.strip().lower() not in used_questions:
                question_only_pool.append(ex)
        
        actual_num_q = min(num_questions, len(question_only_pool))
        question_shots = random.sample(question_only_pool, actual_num_q) if question_only_pool else []
        
        # æ„å»º promptï¼ˆæŒ‰ç…§ AdaCache çš„é¡ºåºï¼‰
        parts = []
        
        # 1. å…ˆæ·»åŠ å¼•å¯¼è¯­ï¼ˆå¦‚æœæœ‰ question-only shotsï¼‰
        if question_shots:
            parts.append("You will be provided Problems similar to the ones below:")
            for ex in question_shots:
                q, _ = self.dataset_handler.format_example_cot(ex)
                parts.append(f"Problem: {q}")
        
        # 2. æ·»åŠ åˆ†éš”ç¬¦å’Œæç¤º
        if fullshots:
            if question_shots:
                parts.append("â€”")  # åˆ†éš”ç¬¦
            parts.append("Now, I am going to give you a series of demonstrations of Problems and Solutions to specify the output format.")
            parts.append("When you respond, think step by step, but your last line must be exactly of the form '#### <final_answer>'.")
            
            # 3. æ·»åŠ  fullshots
            for ex in fullshots:
                q, cot = self.dataset_handler.format_example_cot(ex)
                parts.append(f"Problem: {q}\nSolution: {cot}")
            
            # æ·»åŠ æœ€åçš„åˆ†éš”ç¬¦
            parts.append("â€”")
        
        # 4. æ·»åŠ æµ‹è¯•é—®é¢˜
        if parts:
            prompt = "\n".join(parts) + f"\nProblem: {test_question}\nSolution:"
        else:
            prompt = f"Problem: {test_question}\nSolution:"
        
        return prompt, len(fullshots), len(question_shots)
    
    def _save_pool_info(self, pool_info_path: str):
        """ä¿å­˜ç¤ºä¾‹æ± ä¿¡æ¯åˆ° JSON æ–‡ä»¶"""
        pool_info = {
            "status": "ready",
            "pool_size": len(self.train_pool),
            "dataset": f"{self.dataset_name}/{self.dataset_subset or 'default'}",
            "mode": self.config.mode,
            "selection_method": "random",
            "seed": self.config.seed,
        }
        
        # æ·»åŠ æ¨¡å¼ç‰¹å®šä¿¡æ¯
        if self.config.mode == "cot":
            pool_info["num_shots"] = self.config.num_shots
            pool_info["shot_type"] = "fullshot (question + reasoning + answer)"
        elif self.config.mode == "io":
            pool_info["num_shots"] = self.config.num_shots
            pool_info["shot_type"] = "shortshot (question + answer only)"
        else:  # paper
            pool_info["paper_k_full"] = self.config.paper_k_full
            pool_info["paper_num_questions"] = self.config.paper_num_questions
            pool_info["shot_type"] = "mixed (fullshots + question-only shots)"
        
        # æ·»åŠ ç¤ºä¾‹é¢„è§ˆï¼ˆå‰5ä¸ªï¼‰
        examples_preview = []
        preview_count = min(5, len(self.train_pool))
        
        for idx in range(preview_count):
            try:
                ex = self.train_pool[idx]
                q, cot = self.dataset_handler.format_example_cot(ex)
                answer = self.dataset_handler.extract_gold_answer(ex)
                
                # æˆªæ–­é•¿æ–‡æœ¬
                q_preview = q[:150] + "..." if len(q) > 150 else q
                cot_preview = cot[:150] + "..." if len(cot) > 150 else cot
                
                examples_preview.append({
                    "index": idx,
                    "question": q_preview,
                    "answer_preview": cot_preview if self.config.mode == "cot" else answer
                })
            except Exception as e:
                logging.warning(f"å¤„ç†ç¤ºä¾‹ {idx} å¤±è´¥: {e}")
                continue
        
        pool_info["examples_preview"] = examples_preview
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        try:
            with open(pool_info_path, "w", encoding="utf-8") as f:
                json.dump(pool_info, f, ensure_ascii=False, indent=2)
            logging.info(f"ç¤ºä¾‹æ± ä¿¡æ¯å·²ä¿å­˜: {pool_info_path}")
        except Exception as e:
            logging.warning(f"ä¿å­˜ç¤ºä¾‹æ± ä¿¡æ¯å¤±è´¥: {e}")
    
    def _pick_input_device(self):
        """é€‰æ‹©è¾“å…¥è®¾å¤‡"""
        if hasattr(self.model, 'hf_device_map') and self.model.hf_device_map:
            device_map = self.model.hf_device_map
            first_layer = sorted(device_map.keys())[0]
            return device_map[first_layer]
        elif hasattr(self.model, 'device'):
            return self.model.device
        else:
            return torch.device('cpu')
    
    def _truncate_response(self, response: str) -> str:
        """
        æˆªæ–­æ¨¡å‹è¾“å‡ºï¼Œåªä¿ç•™ç¬¬ä¸€ä¸ªå®Œæ•´ç­”æ¡ˆ
        
        é—®é¢˜ï¼šæ¨¡å‹å¯èƒ½åœ¨å›ç­”å®Œç¬¬ä¸€ä¸ªé—®é¢˜åç»§ç»­ç”Ÿæˆå…¶ä»–å†…å®¹
        è§£å†³ï¼šåœ¨å„ç§ç»“æŸæ ‡è®°å¤„æˆªæ–­
        """
        import re
        
        # æ–¹æ³• 1: åœ¨ "#### " + ç­”æ¡ˆ åæˆªæ–­
        # åŒ¹é…æ¨¡å¼ï¼š#### ç­”æ¡ˆ åé¢å¯èƒ½è·Ÿç€æ¢è¡Œç¬¦å’Œæ–°é—®é¢˜
        match = re.search(r'(####\s*[\-]?\d+(?:\.\d+)?)', response)
        if match:
            end_pos = match.end()
            # æ£€æŸ¥åé¢æ˜¯å¦æœ‰æ–°é—®é¢˜å¼€å§‹
            remaining = response[end_pos:]
            # å¦‚æœåé¢æœ‰ "Problem:" æˆ–é‡å¤å†…å®¹ï¼Œåˆ™æˆªæ–­
            if 'Problem:' in remaining or 'problem:' in remaining.lower():
                response = response[:end_pos].strip()
                return response
        
        # æ–¹æ³• 2: åœ¨ "Problem:" é‡å¤å‡ºç°æ—¶æˆªæ–­
        # æŸ¥æ‰¾ç¬¬äºŒä¸ª "Problem:" çš„ä½ç½®
        first_problem = response.find('Problem:')
        if first_problem != -1:
            second_problem = response.find('Problem:', first_problem + 1)
            if second_problem != -1:
                response = response[:second_problem].strip()
                return response
        
        # æ–¹æ³• 3: åœ¨åŒæ¢è¡Œ + æ–°å†…å®¹å¼€å§‹æ—¶æˆªæ–­
        # æŸ¥æ‰¾ "\n\n" åé¢è·Ÿç€æ–°é—®é¢˜
        parts = response.split('\n\n')
        if len(parts) > 1:
            result_parts = [parts[0]]
            for part in parts[1:]:
                part_lower = part.strip().lower()
                if part_lower.startswith('problem:') or part_lower.startswith('solution:'):
                    break
                result_parts.append(part)
            response = '\n\n'.join(result_parts)
        
        return response.strip()
    
    def evaluate(self, test_set, output_prefix: str) -> Dict:
        """
        è¿è¡Œè¯„ä¼°
        
        Args:
            test_set: æµ‹è¯•é›†
            output_prefix: è¾“å‡ºæ–‡ä»¶å‰ç¼€
        
        Returns:
            metrics: è¯„ä¼°æŒ‡æ ‡
        """
        # å‡†å¤‡è¾“å‡ºæ–‡ä»¶
        jsonl_path = f"{output_prefix}.jsonl"
        meta_path = f"{output_prefix}_metrics.json"
        txt_path = f"{output_prefix}_report.txt"
        prompt_example_path = f"{output_prefix}_prompt_example.txt"
        pool_info_path = f"{output_prefix}_pool_info.json"
        
        # ä¿å­˜ç¤ºä¾‹æ± ä¿¡æ¯
        self._save_pool_info(pool_info_path)
        
        # è¯„ä¼°å¾ªç¯
        logging.info("=" * 60)
        logging.info(f"å¼€å§‹ {self.config.mode.upper()} æ¨¡å¼è¯„ä¼°")
        logging.info("=" * 60)
        
        n_eval = len(test_set)
        system_prompt = self._get_system_prompt()
        
        # ç»Ÿè®¡æŒ‡æ ‡
        total_em_str = 0.0
        total_contains = 0.0
        total_token_f1 = 0.0
        n_numeric_ok = 0
        total_relerr = []
        total_latency = []
        total_in_tok = []
        total_out_tok = []
        total_tps = []
        total_shots_used = []
        
        # ç”¨äºä¿å­˜ç¬¬ä¸€ä¸ª prompt ç¤ºä¾‹
        first_prompt_saved = False
        
        iterator = tqdm(range(n_eval), total=n_eval, desc=f"{self.config.mode.upper()} è¯„ä¼°")
        
        with open(jsonl_path, "w", encoding="utf-8") as fout, \
             torch.inference_mode():
            
            for i in iterator:
                ex = test_set[i]
                q, ref = self.dataset_handler.format_example_cot(ex)
                gold_final = self.dataset_handler.extract_gold_answer(ex)
                
                # æ ¹æ®æ¨¡å¼æ„å»º prompt
                if self.config.mode == "cot":
                    user_content, num_shots = self._build_prompt_cot(q, self.config.num_shots)
                    mode_info = f"CoT-{num_shots}shot"
                elif self.config.mode == "io":
                    user_content, num_shots = self._build_prompt_io(q, self.config.num_shots)
                    mode_info = f"IO-{num_shots}shot"
                else:  # paper
                    user_content, k_full, num_q = self._build_prompt_paper(
                        q, self.config.paper_k_full, self.config.paper_num_questions
                    )
                    num_shots = k_full + num_q
                    mode_info = f"Paper-{k_full}full+{num_q}q"
                
                total_shots_used.append(num_shots)
                
                # æ„å»ºæ¶ˆæ¯
                messages = [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_content}
                ]
                
                try:
                    text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
                except Exception:
                    text = f"{system_prompt}\n\n{user_content}\n"
                
                # ä¿å­˜ç¬¬ä¸€ä¸ª prompt ç¤ºä¾‹
                if not first_prompt_saved:
                    try:
                        with open(prompt_example_path, "w", encoding="utf-8") as f_prompt:
                            f_prompt.write("=" * 80 + "\n")
                            f_prompt.write(f"Shot Baseline ({self.config.mode.upper()}) Prompt ç¤ºä¾‹\n")
                            f_prompt.write("=" * 80 + "\n\n")
                            f_prompt.write(f"æµ‹è¯•é—®é¢˜ç´¢å¼•: {i}\n")
                            f_prompt.write(f"é—®é¢˜: {q[:200]}...\n")
                            f_prompt.write(f"æ¨¡å¼: {mode_info}\n")
                            f_prompt.write(f"Shot æ•°: {num_shots}\n")
                            f_prompt.write("\n" + "=" * 80 + "\n")
                            f_prompt.write("System Prompt\n")
                            f_prompt.write("=" * 80 + "\n")
                            f_prompt.write(system_prompt + "\n\n")
                            f_prompt.write("=" * 80 + "\n")
                            f_prompt.write("User Content\n")
                            f_prompt.write("=" * 80 + "\n")
                            f_prompt.write(user_content + "\n\n")
                            f_prompt.write("=" * 80 + "\n")
                            f_prompt.write("å®Œæ•´ Promptï¼ˆåº”ç”¨ chat template åï¼‰\n")
                            f_prompt.write("=" * 80 + "\n")
                            f_prompt.write(text + "\n\n")
                            f_prompt.write("=" * 80 + "\n")
                            f_prompt.write("é…ç½®ä¿¡æ¯\n")
                            f_prompt.write("=" * 80 + "\n")
                            f_prompt.write(f"æ¨¡å¼: {self.config.mode}\n")
                            if self.config.mode == "paper":
                                f_prompt.write(f"Fullshot æ•°: {self.config.paper_k_full}\n")
                                f_prompt.write(f"Question-only æ•°: {self.config.paper_num_questions}\n")
                            else:
                                f_prompt.write(f"Shot æ•°: {self.config.num_shots}\n")
                            f_prompt.write(f"è¯„æµ‹æ ·æœ¬æ•°: {self.config.eval_samples}\n")
                            f_prompt.write(f"ç”Ÿæˆ tokens: {self.config.gen_tokens}\n")
                            f_prompt.write(f"éšæœºç§å­: {self.config.seed}\n")
                        first_prompt_saved = True
                        logging.info(f"Prompt ç¤ºä¾‹å·²ä¿å­˜: {prompt_example_path}")
                    except Exception as e:
                        logging.warning(f"ä¿å­˜ prompt ç¤ºä¾‹å¤±è´¥: {e}")
                
                # ç¼–ç 
                model_inputs = self.tokenizer([text], return_tensors="pt")
                if getattr(self.tokenizer, "pad_token_id", None) is None:
                    self.tokenizer.pad_token_id = getattr(self.tokenizer, "eos_token_id", None)
                
                # ç§»åŠ¨åˆ°è®¾å¤‡
                input_dev = self._pick_input_device()
                try:
                    model_inputs = model_inputs.to(input_dev)
                except Exception:
                    pass
                
                in_tok = int(model_inputs["input_ids"].shape[1])
                
                # ç”Ÿæˆ
                eos_ids = getattr(self.tokenizer, "eos_token_id", None)
                if isinstance(eos_ids, int):
                    eos_ids = [eos_ids]
                
                # å°è¯•è·å–å…¶ä»– eos token
                if hasattr(self.tokenizer, "convert_tokens_to_ids"):
                     # Qwen ç³»åˆ—å¯èƒ½ä½¿ç”¨ <|im_end|>, <|endoftext|> ç­‰
                     extra_eos = ["<|im_end|>", "<|endoftext|>", "</s>"]
                     for t in extra_eos:
                         tid = self.tokenizer.convert_tokens_to_ids(t)
                         if isinstance(tid, int) and tid != self.tokenizer.unk_token_id:
                             if eos_ids is None:
                                 eos_ids = [tid]
                             elif tid not in eos_ids:
                                 eos_ids.append(tid)
                                 
                gen_kwargs = dict(
                    max_new_tokens=max(1, self.config.gen_tokens),
                    do_sample=False,
                    return_dict_in_generate=True,
                    use_cache=True,
                )
                if eos_ids is not None:
                    gen_kwargs["eos_token_id"] = eos_ids
                
                t0 = time.time()
                outputs = self.model.generate(**model_inputs, **gen_kwargs)
                t1 = time.time()
                
                # è§£ç 
                full_seq = outputs.sequences[0]
                prompt_len = model_inputs["input_ids"].shape[1]
                gen_ids = full_seq[prompt_len:]
                response = self.tokenizer.decode(gen_ids, skip_special_tokens=True)
                
                # ğŸ”¥ æˆªæ–­è¾“å‡ºï¼šåªä¿ç•™ç¬¬ä¸€ä¸ªå®Œæ•´ç­”æ¡ˆ
                response = self._truncate_response(response)
                
                out_tok = int(gen_ids.shape[0])
                latency = (t1 - t0)
                tps = (out_tok / latency) if latency > 0 else float("nan")
                
                # æå–é¢„æµ‹ç­”æ¡ˆ
                pred_final = self.dataset_handler.extract_prediction(response)
                
                # è¯„ä¼°
                compare_results = evaluate_answer(pred_final, gold_final)
                numeric_ok, relerr, em_str, contains, tf1 = (
                    compare_results["numeric_ok"],
                    compare_results["relerr"],
                    compare_results["em_str"],
                    compare_results["contains"],
                    compare_results["tf1"]
                )
                
                # è®°å½•
                record = {
                    "question": q,
                    "reference_answer": ref,
                    "gold_answer": gold_final,
                    "model_output": response,
                    "pred_answer": pred_final,
                    "mode": mode_info,
                    "num_shots": num_shots,
                    "EM_string": em_str,
                    "Contains": contains,
                    "Token_F1": tf1,
                    "Numeric_EM": numeric_ok,
                    "rel_error": relerr,
                    "time_spent": latency,
                    "input_tokens": in_tok,
                    "output_tokens": out_tok,
                    "tokens_per_sec": tps
                }
                fout.write(json.dumps(record, ensure_ascii=False) + "\n")
                
                # ç´¯è®¡
                total_em_str += em_str
                total_contains += contains
                total_token_f1 += tf1
                n_numeric_ok += numeric_ok
                if relerr is not None:
                    total_relerr.append(relerr)
                total_latency.append(latency)
                total_in_tok.append(in_tok)
                total_out_tok.append(out_tok)
                total_tps.append(tps)
        
        # è®¡ç®—æ±‡æ€»æŒ‡æ ‡
        logging.info("=" * 60)
        logging.info("æ±‡æ€»è¯„ä¼°ç»“æœ")
        logging.info("=" * 60)
        
        count = n_eval
        avg_shots = float(np.mean(total_shots_used)) if total_shots_used else 0.0
        relerr_mean = float(np.mean(total_relerr)) if total_relerr else 0.0
        relerr_median = float(np.median(total_relerr)) if total_relerr else 0.0
        
        acc_numeric = n_numeric_ok / count if count else 0.0
        # å®¹å·®å‡†ç¡®ç‡
        if total_relerr:
            acc_tol_1e4 = np.mean([1.0 if (e is not None and e <= 1e-4) else 0.0 for e in total_relerr])
            acc_tol_1e3 = np.mean([1.0 if (e is not None and e <= 1e-3) else 0.0 for e in total_relerr])
            acc_tol_1e2 = np.mean([1.0 if (e is not None and e <= 1e-2) else 0.0 for e in total_relerr])
        else:
            acc_tol_1e4 = 0.0
            acc_tol_1e3 = 0.0
            acc_tol_1e2 = 0.0
        
        em_string = total_em_str / count if count else 0.0
        contains = total_contains / count if count else 0.0
        token_f1 = total_token_f1 / count if count else 0.0
        
        meta = {
            "dataset": f"{self.dataset_name}/{self.dataset_subset or 'default'}",
            "mode": self.config.mode,
            "avg_num_shots": avg_shots,
            "count": count,
            "acc_numeric": acc_numeric,
            "acc_numeric_n": n_numeric_ok,
            "em_string": em_string,
            "contains": contains,
            "token_f1": token_f1,
            "acc_tol_1e-4": acc_tol_1e4,
            "acc_tol_1e-3": acc_tol_1e3,
            "acc_tol_1e-2": acc_tol_1e2,
            "relerr_mean": relerr_mean,
            "relerr_median": relerr_median,
            "total_time_s": float(np.sum(total_latency)) if total_latency else float("nan"),
            "avg_latency_s": float(np.mean(total_latency)) if total_latency else float("nan"),
            "avg_in_tokens": float(np.mean(total_in_tok)) if total_in_tok else float("nan"),
            "avg_out_tokens": float(np.mean(total_out_tok)) if total_out_tok else float("nan"),
            "avg_tokpersec": float(np.mean(total_tps)) if total_tps else float("nan"),
        }
        
        # ä¿å­˜æŒ‡æ ‡
        with open(meta_path, "w", encoding="utf-8") as f:
            json.dump(meta, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜æ–‡æœ¬æŠ¥å‘Š
        with open(txt_path, "w", encoding="utf-8") as f:
            f.write("=" * 60 + "\n")
            f.write(f"Shot Baseline ({self.config.mode.upper()}) è¯„ä¼°æŠ¥å‘Š\n")
            f.write("=" * 60 + "\n\n")
            f.write(f"æ•°æ®é›†: {meta['dataset']}\n")
            f.write(f"æµ‹è¯•æ ·æœ¬æ•°: {count}\n")
            f.write(f"æ¨¡å¼: {self.config.mode}\n")
            f.write(f"å¹³å‡ Shot æ•°: {avg_shots:.1f}\n\n")
            
            f.write("=" * 60 + "\n")
            f.write("å‡†ç¡®ç‡æŒ‡æ ‡\n")
            f.write("=" * 60 + "\n")
            f.write(f"Acc (Numeric-EM): {meta['acc_numeric']:.4f} ({meta['acc_numeric_n']}/{count})\n")
            f.write(f"EM (String): {meta['em_string']:.4f}\n")
            f.write(f"Contains: {meta['contains']:.4f}\n")
            f.write(f"Token-F1: {meta['token_f1']:.4f}\n")
            f.write(f"Acc@1e-4: {meta['acc_tol_1e-4']:.4f}, "
                   f"Acc@1e-3: {meta['acc_tol_1e-3']:.4f}, "
                   f"Acc@1e-2: {meta['acc_tol_1e-2']:.4f}\n")
            f.write(f"RelErr mean/median: {relerr_mean:.6f}/{relerr_median:.6f}\n\n")
            
            f.write("=" * 60 + "\n")
            f.write("æ€§èƒ½æŒ‡æ ‡\n")
            f.write("=" * 60 + "\n")
            f.write(f"æ€»è€—æ—¶ (s): {meta['total_time_s']:.3f}\n")
            f.write(f"å¹³å‡å»¶è¿Ÿ (s): {meta['avg_latency_s']:.3f}\n")
            f.write(f"å¹³å‡è¾“å…¥/è¾“å‡º tokens: {meta['avg_in_tokens']:.1f}/{meta['avg_out_tokens']:.1f}\n")
            f.write(f"å¹³å‡ tokens/s: {meta['avg_tokpersec']:.2f}\n\n")
            
            f.write("=" * 60 + "\n")
            f.write("è¾“å‡ºæ–‡ä»¶\n")
            f.write("=" * 60 + "\n")
            f.write(f"JSONL: {jsonl_path}\n")
            f.write(f"Metrics: {meta_path}\n")
            f.write(f"Pool Info: {pool_info_path}\n")
            f.write(f"Prompt Example: {prompt_example_path}\n")
        
        logging.info(f"âœ“ è¯„ä¼°å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° {output_prefix}*")
        logging.info(f"  - å‡†ç¡®ç‡ (Numeric-EM): {meta['acc_numeric']:.4f}")
        logging.info(f"  - å¹³å‡ Shot æ•°: {avg_shots:.1f}")
        
        return meta
